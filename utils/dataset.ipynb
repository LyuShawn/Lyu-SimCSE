{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析数据集\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse.txt'\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print('total lines:', len(lines))\n",
    "\n",
    "# 统计句子长度\n",
    "lengths = [len(line.split()) for line in lines]\n",
    "print('max length:', max(lengths))\n",
    "print('min length:', min(lengths))\n",
    "print('avg length:', np.mean(lengths))\n",
    "print('median length:', np.median(lengths))\n",
    "\n",
    "# 统计句子长度超过32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''A surgeon interviewed by Australia's \"A Current Affair\" television show criticized the marketing of the supplement, stating that customers are \"wasting their money, and for this product a large amount of money; and secondly, they may be led to believe they don’t need to take their effective treatments for conditions they may actually have.” Despite this, Wilson claims that the \"purple powder\" can help elderly people \"keep their vibrations up\", and at one performance she invited an audience member to speak with her about the \"best thing for scar tissue\" off camera, so that \"trading standards don't become all uppity.\"'''\n",
    "print(len(s))\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 使用encode方法\n",
    "input_ids = tokenizer.encode(s)\n",
    "\n",
    "# 打印生成的token数量\n",
    "print(f\"Token count: {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "template = 'This sentence : \\'{sentence}\\' means [MASK].'\n",
    "\n",
    "# template =template.replace('[MASK]', tokenizer.mask_token)\n",
    "prompt_prefix = template.split('{sentence}')[0]    \n",
    "prompt_suffix = template.split('{sentence}')[1]\n",
    "\n",
    "print(prompt_prefix)\n",
    "print(prompt_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理 做实体链接和实体消歧\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "\n",
    "# 初始化 BERT 模型和 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 输入句子\n",
    "sentence = \"She picked a ripe, juicy apple from the tree and took a big bite, savoring its crisp sweetness.\"\n",
    "# sentence = \"Apple recently unveiled its latest iPhone model, showcasing cutting-edge technology and sleek design that captivated tech enthusiasts worldwide.\"\n",
    "target_entity = \"Apple\"\n",
    "entity_description = \"Apple is a sweet, crisp fruit from the apple tree (Malus domestica), enjoyed worldwide for its flavor and nutritional benefits, including fiber and vitamin C.\"\n",
    "# entity_description = \"Apple is a company that designs, manufactures, and markets consumer electronics, computer software, and online services.\"\n",
    "\n",
    "\n",
    "# Token 化\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "entity_tokens = tokenizer(target_entity)\n",
    "\n",
    "entity_id = entity_tokens['input_ids'][1:-1]  # 去掉 [CLS] 和 [SEP]\n",
    "\n",
    "# 扩展一维\n",
    "entity_id = torch.tensor(entity_id).unsqueeze(0)\n",
    "\n",
    "# 获取 BERT 的所有 token 嵌入\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "mask = input_ids == entity_id\n",
    "\n",
    "# 提取并聚合实体嵌入\n",
    "entity_embeddings = last_hidden_states[inputs['input_ids'] == entity_id]\n",
    "\n",
    "entity_description_inputs = tokenizer(entity_description, return_tensors=\"pt\")\n",
    "entity_description_outputs = model(**entity_description_inputs)\n",
    "# 获取cls token的嵌入\n",
    "# 如果有entity_id的话，就取entity_id的嵌入\n",
    "entity_mask = entity_description_inputs['input_ids'] == entity_id\n",
    "if entity_mask.sum() > 0:\n",
    "    entity_description_emb = entity_description_outputs.last_hidden_state[entity_mask]\n",
    "    # 有可能有多个实体，取第一个\n",
    "    entity_description_emb = entity_description_emb[0]\n",
    "else:\n",
    "    entity_description_emb = entity_description_outputs.last_hidden_state[:, 0, :]\n",
    "# 计算实体描述和实体的相似度\n",
    "cos_sim = F.cosine_similarity(entity_embeddings, entity_description_emb, dim=-1)\n",
    "\n",
    "print(cos_sim.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出文件\n",
    "output_file = '../data/wiki1m_for_simcse_ner_entity_linking.json'\n",
    "\n",
    "with open(entity_dict_file, 'r') as f:  \n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:28<00:00, 69.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 做实体链接\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化 BERT 模型和 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 实体置信度阈值\n",
    "entity_reg_threshold = 0.8\n",
    "\n",
    "# 加载外部实体描述数据集\n",
    "entity_dict_file = '../data/wiki1m_for_simcse_ner_entity_dict.json'\n",
    "\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# 输入句子\n",
    "dataset = load_dataset('json', data_files='../data/wiki1m_for_simcse_ner.json')['train']\n",
    "\n",
    "dataset = dataset.select(range(2000))\n",
    "\n",
    "\n",
    "item_list = []\n",
    "for item in tqdm(dataset):\n",
    "    sentence, entity_list = item['text'], item['entities']\n",
    "\n",
    "    s_inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    s_output = model(**s_inputs)\n",
    "    s_last_hidden_states = s_output.last_hidden_state\n",
    "\n",
    "    # 输出策略\n",
    "    # s_output = s_output.last_hidden_state[:, 0, :]  # 取CLS\n",
    "\n",
    "    sentence_item = {'sentence': sentence, 'entity_list': [],'ner_entity_list': entity_list}\n",
    "    for entity in entity_list:\n",
    "        # 实体置信度过滤\n",
    "        if entity['confidence'] < entity_reg_threshold:\n",
    "            continue\n",
    "\n",
    "        entity_text = entity['text']\n",
    "\n",
    "        entity_text_hash = hash(entity_text)\n",
    "\n",
    "        entity_knowledge = entity_dict.get(entity_text_hash, [])\n",
    "        if not entity_knowledge:\n",
    "            continue\n",
    "\n",
    "        # 获取实体的id\n",
    "        entity_inputs = tokenizer(entity_text)\n",
    "        # 在最后一维去掉 [CLS] 和 [SEP]\n",
    "        entity_inputs = entity_inputs['input_ids'][1]  # 去掉 [CLS] 和 [SEP]，取第一个\n",
    "        entity_id = torch.tensor(entity_inputs).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "        # 实体描述的列表\n",
    "        entity_description_list = [item['description'] for item in entity_knowledge if item.get('description', '')]\n",
    "        if not entity_description_list:\n",
    "            continue\n",
    "        # 实体描述的文本token化\n",
    "        entity_description_inputs = tokenizer(entity_description_list, return_tensors=\"pt\", padding=True).to(device) # [num_descriptions, max_length]\n",
    "        entity_description_outputs = model(**entity_description_inputs) # [num_descriptions, max_length, hidden_size]\n",
    "\n",
    "\n",
    "        # 如果inputs中有entity_id的话，就取entity_id的嵌入，否则取cls token的嵌入\n",
    "        # entity_mask = entity_description_inputs['input_ids'] == entity_inputs # [num_descriptions, max_length]\n",
    "        entity_description_outputs = entity_description_outputs.last_hidden_state[:, 0, :]  # [num_descriptions, hidden_size]\n",
    "\n",
    "        # 获取原句子的实体嵌入\n",
    "        s_entity_output = s_last_hidden_states[s_inputs['input_ids'] == entity_id]\n",
    "        # 有可能有多个实体，取第一个\n",
    "        if s_entity_output.shape[0] > 1:\n",
    "            s_entity_output = s_entity_output[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # 计算实体描述和实体的相似度\n",
    "        cos_sim = F.cosine_similarity(s_entity_output, entity_description_outputs, dim=-1) # [num_descriptions]\n",
    "        # 获取最相似的实体描述\n",
    "        max_index = cos_sim.argmax().item()\n",
    "        max_cos_sim = cos_sim.max().item()\n",
    "        max_entity_description = entity_description_list[max_index]\n",
    "\n",
    "        sentence_item['entity_list'].append({'entity': entity_text, 'description': max_entity_description, 'similarity': max_cos_sim})\n",
    "\n",
    "    item_list.append(sentence_item)\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(item_list, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
