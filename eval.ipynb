{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io, os\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set PATHs\n",
    "PATH_TO_SENTEVAL = './SentEval'\n",
    "PATH_TO_DATA = './SentEval/data'\n",
    "\n",
    "# Import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval setting\n",
    "base_path = 'result/'\n",
    "eval_list = ['3','4','5','6','7','8','9','10','11','12','13']\n",
    "\n",
    "pooler = 'cls'\n",
    "task_set = 'sts'\n",
    "mode = 'test'\n",
    "\n",
    "times = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(task_names, scores):\n",
    "    tb = PrettyTable()\n",
    "    tb.field_names = task_names\n",
    "    tb.add_row(scores)\n",
    "    logging.info(tb)\n",
    "\n",
    "def eval(model_name_or_path,pooler,mode,task_set,epoch,tasks=None,):\n",
    "    # setup log\n",
    "    log_path = os.path.join(model_name_or_path,'eval_logs')\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "    log_file = os.path.join(log_path, f'{epoch}.log')\n",
    "    # 设置 log\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=log_file, format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "        \n",
    "    # load model\n",
    "    model = AutoModel.from_pretrained(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "     # Set up the tasks\n",
    "    if task_set == 'sts':\n",
    "        tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    elif task_set == 'transfer':\n",
    "        tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "    elif task_set == 'full':\n",
    "        tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "        tasks += ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "    \n",
    "    # Set params for SentEval\n",
    "    if mode == 'dev' or mode == 'fasttest':\n",
    "        # Fast mode\n",
    "        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                         'tenacity': 3, 'epoch_size': 2}\n",
    "    elif mode == 'test':\n",
    "        # Full mode\n",
    "        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "        params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n",
    "                                         'tenacity': 5, 'epoch_size': 4}\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "     # SentEval prepare and batcher\n",
    "    def prepare(params, samples):\n",
    "        return\n",
    "    \n",
    "    def batcher(params, batch, max_length=None):\n",
    "        # Handle rare token encoding issues in the dataset\n",
    "        if len(batch) >= 1 and len(batch[0]) >= 1 and isinstance(batch[0][0], bytes):\n",
    "            batch = [[word.decode('utf-8') for word in s] for s in batch]\n",
    "\n",
    "        sentences = [' '.join(s) for s in batch]\n",
    "\n",
    "        # Tokenization\n",
    "        if max_length is not None:\n",
    "            batch = tokenizer.batch_encode_plus(\n",
    "                sentences,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "        else:\n",
    "            batch = tokenizer.batch_encode_plus(\n",
    "                sentences,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "            )\n",
    "\n",
    "        # Move to the correct device\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        \n",
    "        # Get raw embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            pooler_output = outputs.pooler_output\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "         # Apply different poolers\n",
    "        if pooler == 'cls':\n",
    "            # There is a linear+activation layer after CLS representation\n",
    "            return pooler_output.cpu()\n",
    "        elif pooler == 'cls_before_pooler':\n",
    "            return last_hidden[:, 0].cpu()\n",
    "        elif pooler == \"avg\":\n",
    "            return ((last_hidden * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)).cpu()\n",
    "        elif pooler == \"avg_first_last\":\n",
    "            first_hidden = hidden_states[1]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((first_hidden + last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n",
    "            return pooled_result.cpu()\n",
    "        elif pooler == \"avg_top2\":\n",
    "            second_last_hidden = hidden_states[-2]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n",
    "            return pooled_result.cpu()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for task in tasks:\n",
    "        se = senteval.engine.SE(params, batcher, prepare)\n",
    "        result = se.eval(task)\n",
    "        results[task] = result\n",
    "\n",
    "    # Print evaluation results\n",
    "    if mode == 'dev':\n",
    "        logging.info(\"------ %s ------\" % (mode))\n",
    "\n",
    "        task_names = []\n",
    "        scores = []\n",
    "        for task in ['STSBenchmark', 'SICKRelatedness']:\n",
    "            task_names.append(task)\n",
    "            if task in results:\n",
    "                scores.append(\"%.2f\" % (results[task]['dev']['spearman'][0] * 100))\n",
    "            else:\n",
    "                scores.append(\"0.00\")\n",
    "        print_table(task_names, scores)\n",
    "\n",
    "        task_names = []\n",
    "        scores = []\n",
    "        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n",
    "            task_names.append(task)\n",
    "            if task in results:\n",
    "                scores.append(\"%.2f\" % (results[task]['devacc']))    \n",
    "            else:\n",
    "                scores.append(\"0.00\")\n",
    "        task_names.append(\"Avg.\")\n",
    "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
    "        print_table(task_names, scores)\n",
    "\n",
    "    elif mode == 'test' or mode == 'fasttest':\n",
    "        logging.info(\"------ %s ------\" % (mode))\n",
    "\n",
    "        task_names = []\n",
    "        scores = []\n",
    "        for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']:\n",
    "            task_names.append(task)\n",
    "            if task in results:\n",
    "                if task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16']:\n",
    "                    scores.append(\"%.2f\" % (results[task]['all']['spearman']['all'] * 100))\n",
    "                else:\n",
    "                    scores.append(\"%.2f\" % (results[task]['test']['spearman'].correlation * 100))\n",
    "            else:\n",
    "                scores.append(\"0.00\")\n",
    "        task_names.append(\"Avg.\")\n",
    "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
    "        print_table(task_names, scores)\n",
    "\n",
    "        task_names = []\n",
    "        scores = []\n",
    "        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n",
    "            task_names.append(task)\n",
    "            if task in results:\n",
    "                scores.append(\"%.2f\" % (results[task]['acc']))    \n",
    "            else:\n",
    "                scores.append(\"0.00\")\n",
    "        task_names.append(\"Avg.\")\n",
    "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
    "        print_table(task_names, scores)\n",
    "    return results\n",
    "\n",
    "def process_result(result, task_scores):\n",
    "    sum = 0\n",
    "    for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']:\n",
    "        if task in result:\n",
    "            if task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16']:\n",
    "                task_scores[task].append(result[task]['all']['spearman']['all'])\n",
    "                sum += result[task]['all']['spearman']['all']\n",
    "            else:\n",
    "                task_scores[task].append(result[task]['test']['spearman'].correlation)\n",
    "                sum += result[task]['test']['spearman'].correlation\n",
    "    task_scores['avg'].append(sum / 7)\n",
    "\n",
    "def calculate_average(task_scores):\n",
    "    avg_scores = {'STS12': [], 'STS13': [], 'STS14': [], 'STS15': [], 'STS16': [], 'STSBenchmark': [], 'SICKRelatedness': [],'avg':[]}\n",
    "    for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness','avg']:\n",
    "        avg_scores[task] = sum(task_scores[task]) / len(task_scores[task])\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ['3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
      "start evaluating 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/3 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "./SentEval/senteval/sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "./SentEval/senteval/sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/3 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/3 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/4 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/4 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/4 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/5 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/5 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/5 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/6 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/6 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/6 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/7 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/7 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/7 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/8 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/8 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/8 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/9 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/9 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/9 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/10 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/10 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/10 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/11 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/11 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/11 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/12 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/12 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/12 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/13 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/13 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at result/13 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# start eval\n",
    "\n",
    "print(f'evaluating {eval_list}')\n",
    "for item in eval_list:\n",
    "    print(f'start evaluating {item}')\n",
    "    task_scores = {'STS12': [], 'STS13': [], 'STS14': [], 'STS15': [], 'STS16': [], 'STSBenchmark': [], 'SICKRelatedness': [],'avg':[]}\n",
    "    item_path = os.path.join(base_path,item)\n",
    "    for index in range(times):\n",
    "        result = eval(model_name_or_path=item_path,pooler=pooler,mode=mode,task_set=task_set,epoch=index)\n",
    "        process_result(result, task_scores)\n",
    "    avg_scores = calculate_average(task_scores)\n",
    "    scores = {'avg_scores':avg_scores,'task_scores':task_scores}\n",
    "    with open(os.path.join(item_path,'avg_scores.json'),'w') as f:\n",
    "        json.dump(scores,f, indent=4, sort_keys=True)\n",
    "print('done')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
