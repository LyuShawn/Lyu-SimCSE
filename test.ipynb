{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "# model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "bs = 64\n",
    "\n",
    "list1 = sent_list[:bs]\n",
    "list2 = random.sample(sent_list, bs)\n",
    "\n",
    "sent_list = list1 * 2 + list2 * 2   # list1 list1 list2 list2\n",
    "\n",
    "print(len(sent_list))\n",
    "\n",
    "inputs = tokenizer(sent_list, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs.to(device))\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state  # (bs*4, seq_len, hidden_size)\n",
    "\n",
    "pooler_output = outputs.pooler_output  # (bs*4, hidden_size)\n",
    "pooler_output = pooler_output.view (4, bs, -1)  # (4, bs, hidden_size)\n",
    "\n",
    "z1 ,z2 ,z3, z4 = pooler_output\n",
    "\n",
    "class Similarity(nn.Module):\n",
    "\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n",
    "\n",
    "# sim = Similarity(1)\n",
    "\n",
    "# cos_sim_12 = sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "# 打印相似度\n",
    "# print(cos_sim_12)\n",
    "\n",
    "# cos_sim_34 = sim(z3.unsqueeze(1), z4.unsqueeze(0))\n",
    "\n",
    "# loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "# labels = torch.arange(cos_sim_12.size(0)).long().to(device)\n",
    "\n",
    "# loss1 = loss_fct(cos_sim_12, labels)\n",
    "# loss2 = loss_fct(cos_sim_34, labels)\n",
    "\n",
    "# print(loss1, loss2)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 对Z1可视化\n",
    "# 使用 t-SNE 降维\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(z3.cpu().numpy())\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=1, alpha=0.5)\n",
    "plt.title('Uniformity Visualization (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "# model_name = 'bert-base-uncased'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# sent_list = random.sample(sent_list, 1000)\n",
    "\n",
    "sentence_embeddings = []\n",
    "with torch.no_grad():\n",
    "    bs = 1024\n",
    "    for i in tqdm(range(0, len(sent_list), bs)):\n",
    "        list = sent_list[i:i+bs]\n",
    "        inputs = tokenizer(list, padding=True, truncation=True, return_tensors='pt')\n",
    "        outputs = model(**inputs.to(device))\n",
    "        sentence_embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "    # outputs = model(**inputs.to(device))\n",
    "\n",
    "# sentence_embeddings = outputs.last_hidden_state[:, 0, :]  # 取 [CLS] 的输出作为句子向量\n",
    "\n",
    "\n",
    "# 假设你有 1M 个句子的嵌入，每个句子的维度是 768\n",
    "# sentence_embeddings = torch.randn(1000000, 768)  # 模拟1M个句子的嵌入\n",
    "\n",
    "# 计算目标簇数\n",
    "batch_size = 64\n",
    "num_samples = sentence_embeddings.shape[0]\n",
    "num_clusters = num_samples // batch_size  # 计算所需的簇的数量\n",
    "\n",
    "# 将张量转换为 numpy 数组\n",
    "sentence_embeddings_np = sentence_embeddings.cpu().numpy()\n",
    "\n",
    "# 使用 MiniBatchKMeans 聚类\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, batch_size=batch_size, n_init=10)\n",
    "minibatch_kmeans.fit(sentence_embeddings_np)\n",
    "\n",
    "# 获取每个句子的簇标签\n",
    "labels = minibatch_kmeans.labels_\n",
    "\n",
    "# 查看每个簇的标签\n",
    "print(\"聚类标签:\", labels[:100])  # 查看前 100 个样本的标签\n",
    "# 打印每个簇的大小\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(\"每个簇的大小:\", dict(zip(unique, counts)))\n",
    "\n",
    "# 获取每个簇的中心（每个簇的代表句子）\n",
    "# centroids = minibatch_kmeans.cluster_centers_\n",
    "\n",
    "# 查看每个簇的中心\n",
    "# print(\"每个簇的中心（代表句子）:\", centroids[:5])  # 只查看前 5 个簇中心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:09<00:00, 109.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 278998\n",
      "min: 1\n",
      "mean: 5.754275422363394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from knowledge.retrieval import retrieval_knowledge_title\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "# sent_list = sent_list[:1000]\n",
    "# sent_list = random.sample(sent_list, 1000)\n",
    "bs = 1000\n",
    "\n",
    "sent_dict = {}\n",
    "\n",
    "for i in tqdm(range(0, len(sent_list), bs)):\n",
    "    list = sent_list[i:i+bs]\n",
    "    result = retrieval_knowledge_title(list)\n",
    "\n",
    "    for sent, title in zip(list, result):\n",
    "        key = \",\".join(title)\n",
    "        if key not in sent_dict:\n",
    "            sent_dict[key] = []\n",
    "        sent_dict[key].append(sent)\n",
    "\n",
    "length_list = [len(v) for k, v in sent_dict.items()]\n",
    "print('max:', max(length_list))\n",
    "print('min:', min(length_list))\n",
    "print('mean:', sum(length_list) / len(length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:08<00:00, 115.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721003\n"
     ]
    }
   ],
   "source": [
    "# 生成新的训练数据\n",
    "\n",
    "from knowledge.retrieval import retrieval_knowledge_title\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "new_sent_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(sent_list), bs)):\n",
    "    list = sent_list[i:i+bs]\n",
    "    result = retrieval_knowledge_title(list)\n",
    "    for sent, title in zip(list, result):\n",
    "        if title:\n",
    "            new_sent_list.append(sent)\n",
    "\n",
    "print(len(new_sent_list))\n",
    "output_file = 'data/wiki1m_for_simcse_remove_empty_title.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(new_sent_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
