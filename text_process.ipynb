{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算语料库的平均长度\n",
    "\n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "\n",
    "total_length = 0\n",
    "total_sentences = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        total_length += len(words)\n",
    "        total_sentences += 1\n",
    "\n",
    "average_length = total_length / total_sentences\n",
    "print(average_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词测试\n",
    "import spacy\n",
    "\n",
    "# 加载英语语言模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 分词示例\n",
    "text = \"South Australia (SA)  has a unique position in Australia's history as, unlike the other states which were founded as colonies, South Australia began as a self governing province Many were attracted to this and Adelaide and SA developed as an independent and free thinking state.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# # 关键词提取示例\n",
    "# print(\"\\n关键词提取:\")\n",
    "# keywords = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "# # 选择关键词列表中的第一个词作为最关键的词\n",
    "# most_important_word = keywords[0] if keywords else None\n",
    "# print(most_important_word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://59.77.134.205:9200\")\n",
    "\n",
    "index_name = \"wikidata-latest\"\n",
    "\n",
    "query = {\"query\": {\"match\": {\"content\": \"China\"}}}\n",
    "result = es.search(index=index_name, body=query)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy记录\n",
    "\n",
    "不同模型有不同精度\n",
    "\n",
    "#### [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm)\n",
    "\n",
    "支持的实体类型\n",
    "\n",
    "<img src=\"https://lyu-oss.oss-cn-beijing.aliyuncs.com/img-bed/image-20231227161004504.png\" alt=\"image-20231227161004504\" style=\"zoom:50%;\" />\n",
    "\n",
    "* CARDINAL -- 数字 【'九百多', '8000', '八百'，'1111.01'】\n",
    "* DATE -- 大粒度时间，时间段 【 '今年', '明天', '今天', '国庆期间', '3天', '10天'， '三年前'】\n",
    "* **EVENT -- 事件 【'伦敦奥运会', '世界杯','第14届中国国际工业博览会', '深圳市五届人大二次会议'】**\n",
    "* **FAC -- 小地点 【'轻轨1号线锡北运河站', '万达广场', '乐购超市'，'永盛大酒店', '110岗亭'】**\n",
    "* **GPE -- 地点 【'美国', '加拿大', '北京', '中国',】**\n",
    "* **LANGUAGE -- 语言 【'英语', '汉语', '上海话', '中文'】**\n",
    "* **LAW -- 规章制度 【'青少年犯罪法', '阿鲁巴决议', '反托拉斯法'】**\n",
    "* **LOC -- 大地点 【'欧洲人', '欧洲', '亚洲', '天山山脉', '巴尔斯卡乌尼河''月球', '火星'】**\n",
    "* MONEY -- 货币 【'￥8000', '9200', '60元', '3000美金'】部分省略货币单位也能识别\n",
    "* **NORP -- 人物、地点 【'德国', '中国', '中国人','韩版', '日媒', '日本', '扬州'】**\n",
    "* ORDINAL -- 顺序 【'首', '第六', '第二', '第一','第几'】\n",
    "* **ORG -- 组织 【'杭州江干区公安分局'，'LG', '三星', '苹果'，'中国移动', '央行'，'中央社'， '外交部'】**\n",
    "* PERCENT -- 比率 【'0.2%', '0.2%', '48%'】\n",
    "* **PERSON -- 人物 【'栾丽娜', '斯蒂芬·弗雷斯', '栗元广', '小雨', '小银狐'】**\n",
    "* **PRODUCT -- 产品 【'Android', 'iOS'， '金龙鱼', 'UCWeb的浏览器'】**\n",
    "* QUANTITY -- 量级 【'87,000', '46.522吨', '48248.8千克', '0.23点'，'5.3级', '4200公里' 】\n",
    "* TIME -- 时间 【'十分钟', '下午', '七点', '今晚'】\n",
    "* **WORK_OF_ART -- 艺术品 【'刺客信条', '富春山居图', '有一说一'，'探索•发现', '定窑考工记'】**\n",
    "\n",
    "事件、地点、语言、规章制度、人物、组织、产品、艺术品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000000it [1:23:15, 200.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE: 373869\n",
      "NORP: 152530\n",
      "ORG: 532072\n",
      "WORK_OF_ART: 80636\n",
      "FAC: 37106\n",
      "PERSON: 505931\n",
      "LOC: 56057\n",
      "PRODUCT: 22275\n",
      "LAW: 6742\n",
      "EVENT: 29668\n",
      "LANGUAGE: 8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#分词统计\n",
    "# 统计语料库中各种类型的实体数量\n",
    "\n",
    "# 事件、地点、语言、规章制度、人物、组织、产品、艺术品\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "type_list = ['EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'NORP', 'ORG', 'PERSON', 'PRODUCT', 'WORK_OF_ART']\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "\n",
    "type_counts = Counter()\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for index, line in tqdm(enumerate(file)):\n",
    "        words = line.strip().split()\n",
    "        text = ' '.join(words)\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in type_list:\n",
    "                type_counts[ent.label_] += 1\n",
    "    \n",
    "\n",
    "for type_name, count in type_counts.items():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "\n",
    "\n",
    "# GPE: 373869\n",
    "# NORP: 152530\n",
    "# ORG: 532072\n",
    "# WORK_OF_ART: 80636\n",
    "# FAC: 37106\n",
    "# PERSON: 505931\n",
    "# LOC: 56057\n",
    "# PRODUCT: 22275\n",
    "# LAW: 6742\n",
    "# EVENT: 29668\n",
    "# LANGUAGE: 8100\n",
    "# sum: 1,763,006\n",
    "    \n",
    "# 事件:P31: Q1656682\n",
    "# 地点:P31: Q2221906\n",
    "# 语言:P31: Q34770\n",
    "# 规章制度:P31: Q22097341\n",
    "# 人物:P31: Q5\n",
    "# 组织:P31: Q43229\n",
    "# 产品:P31: Q2424752\n",
    "# 艺术品:P31: Q838948"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
