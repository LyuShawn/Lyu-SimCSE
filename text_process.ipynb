{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.432477\n"
     ]
    }
   ],
   "source": [
    "# 计算语料库的平均长度\n",
    "\n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "\n",
    "total_length = 0\n",
    "total_sentences = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        total_length += len(words)\n",
    "        total_sentences += 1\n",
    "\n",
    "average_length = total_length / total_sentences\n",
    "print(average_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将语料库分片\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "output_dir = 'data/wiki1m_for_simcse_splited'\n",
    "chunks = 10\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        output_file_path = os.path.join(output_dir, str(i % chunks) + '.txt')\n",
    "        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "            output_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://59.77.134.205:9200\")\n",
    "\n",
    "index_name = \"wikidata-latest\"\n",
    "\n",
    "query = {\"query\": {\"match\": {\"content\": \"China\"}}}\n",
    "result = es.search(index=index_name, body=query)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy记录\n",
    "\n",
    "不同模型有不同精度\n",
    "\n",
    "#### [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm)\n",
    "\n",
    "支持的实体类型\n",
    "\n",
    "<img src=\"https://lyu-oss.oss-cn-beijing.aliyuncs.com/img-bed/image-20231227161004504.png\" alt=\"image-20231227161004504\" style=\"zoom:50%;\" />\n",
    "\n",
    "* CARDINAL -- 数字 【'九百多', '8000', '八百'，'1111.01'】\n",
    "* DATE -- 大粒度时间，时间段 【 '今年', '明天', '今天', '国庆期间', '3天', '10天'， '三年前'】\n",
    "* **EVENT -- 事件 【'伦敦奥运会', '世界杯','第14届中国国际工业博览会', '深圳市五届人大二次会议'】**\n",
    "* **FAC -- 小地点 【'轻轨1号线锡北运河站', '万达广场', '乐购超市'，'永盛大酒店', '110岗亭'】**\n",
    "* **GPE -- 地点 【'美国', '加拿大', '北京', '中国',】**\n",
    "* **LANGUAGE -- 语言 【'英语', '汉语', '上海话', '中文'】**\n",
    "* **LAW -- 规章制度 【'青少年犯罪法', '阿鲁巴决议', '反托拉斯法'】**\n",
    "* **LOC -- 大地点 【'欧洲人', '欧洲', '亚洲', '天山山脉', '巴尔斯卡乌尼河''月球', '火星'】**\n",
    "* MONEY -- 货币 【'￥8000', '9200', '60元', '3000美金'】部分省略货币单位也能识别\n",
    "* **NORP -- 人物、地点 【'德国', '中国', '中国人','韩版', '日媒', '日本', '扬州'】**\n",
    "* ORDINAL -- 顺序 【'首', '第六', '第二', '第一','第几'】\n",
    "* **ORG -- 组织 【'杭州江干区公安分局'，'LG', '三星', '苹果'，'中国移动', '央行'，'中央社'， '外交部'】**\n",
    "* PERCENT -- 比率 【'0.2%', '0.2%', '48%'】\n",
    "* **PERSON -- 人物 【'栾丽娜', '斯蒂芬·弗雷斯', '栗元广', '小雨', '小银狐'】**\n",
    "* **PRODUCT -- 产品 【'Android', 'iOS'， '金龙鱼', 'UCWeb的浏览器'】**\n",
    "* QUANTITY -- 量级 【'87,000', '46.522吨', '48248.8千克', '0.23点'，'5.3级', '4200公里' 】\n",
    "* TIME -- 时间 【'十分钟', '下午', '七点', '今晚'】\n",
    "* **WORK_OF_ART -- 艺术品 【'刺客信条', '富春山居图', '有一说一'，'探索•发现', '定窑考工记'】**\n",
    "\n",
    "事件、地点、语言、规章制度、人物、组织、产品、艺术品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词统计\n",
    "# 统计语料库中各种类型的实体数量\n",
    "\n",
    "# 事件、地点、语言、规章制度、人物、组织、产品、艺术品\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "type_list = ['EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'NORP', 'ORG', 'PERSON', 'PRODUCT', 'WORK_OF_ART']\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "\n",
    "type_counts = Counter()\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for index, line in tqdm(enumerate(file)):\n",
    "        words = line.strip().split()\n",
    "        text = ' '.join(words)\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in type_list:\n",
    "                type_counts[ent.label_] += 1\n",
    "    \n",
    "\n",
    "for type_name, count in type_counts.items():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "# GPE: 373869\n",
    "# NORP: 152530\n",
    "# ORG: 532072\n",
    "# WORK_OF_ART: 80636\n",
    "# FAC: 37106\n",
    "# PERSON: 505931\n",
    "# LOC: 56057\n",
    "# PRODUCT: 22275\n",
    "# LAW: 6742\n",
    "# EVENT: 29668\n",
    "# LANGUAGE: 8100\n",
    "# sum: 1,763,006\n",
    "    \n",
    "# 事件:P31: Q1656682\n",
    "# 地点:P31: Q2221906\n",
    "# 语言:P31: Q34770\n",
    "# 规章制度:P31: Q22097341\n",
    "# 人物:P31: Q5\n",
    "# 组织:P31: Q43229\n",
    "# 产品:P31: Q2424752\n",
    "# 艺术品:P31: Q838948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es检索测试\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=\"http://59.77.134.205:9200\")\n",
    "index_name = \"wikidata-filter\"\n",
    "\n",
    "\n",
    "# 执行精确匹配查询\n",
    "exact_match_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"enwiki_title.keyword\": {\n",
    "                \"query\": \"Taylor Swift\",\n",
    "                \"operator\": \"and\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"description\"]\n",
    "\n",
    "}\n",
    "\n",
    "result = es.search(index=index_name, body=exact_match_query)\n",
    "print(result['hits']['hits'][0]['_source']['description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [3:39:52<00:00, 75.80it/s]    \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "spacy_tool = spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "file_path = 'data/wiki1m_for_simcse.txt'\n",
    "output_path = 'data/wiki1m_for_simcse_person_bracket.txt'\n",
    "total_lines = 1000000\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://59.77.134.205:9200\")\n",
    "index_name = \"wikidata-filter\"\n",
    "\n",
    "type_list = ['PERSON','LANGUAGE','ORG']\n",
    "\n",
    "def search_entity_description(entity_name):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"enwiki_title.keyword\": {\n",
    "                    \"query\": entity_name,\n",
    "                    \"operator\": \"and\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"description\"]\n",
    "    }\n",
    "    result = es.search(index=index_name, body=query)\n",
    "    if len(result['hits']['hits']) > 0:\n",
    "        return result['hits']['hits'][0]['_source']['description']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_line(line):\n",
    "    doc = spacy_tool(line)\n",
    "    new_line = line.strip()  # 保留原句\n",
    "    replaced_entities = set()  # 用集合存储已经替换过的实体\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in type_list and ent.text not in replaced_entities:\n",
    "            description = search_entity_description(ent.text)\n",
    "            if description:\n",
    "                new_line = new_line.replace(ent.text, f\"{ent.text} ({description})\")\n",
    "                replaced_entities.add(ent.text)  # 添加已替换的实体到集合中\n",
    "    return new_line\n",
    "\n",
    "count = 0\n",
    "with open(file_path, 'r', encoding='utf-8') as file, open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for result_line in tqdm(executor.map(process_line, file), total=total_lines):\n",
    "            output_file.write(result_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据插入es\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from qwikidata.json_dump import WikidataJsonDump\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.utils import dump_entities_to_json\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://localhost:9200\")\n",
    "index_name = \"wikidata-filter\"\n",
    "es.indices.create(index=index_name,ignore=400)\n",
    "\n",
    "wjd_dump_path = \"wikidata\\wikidata-20231215-all.json.bz2\"\n",
    "wjd = WikidataJsonDump(wjd_dump_path)\n",
    "\n",
    "batch_size=10000\n",
    "batch_num=0\n",
    "item_list=[]\n",
    "\n",
    "property_list = set(['Q1656682','Q2221906','Q34770','Q22097341','Q5','Q43229','Q2424752','Q838948'])\n",
    "\n",
    "for ii, entity_dict in tqdm(enumerate(wjd)):\n",
    "    if entity_dict[\"type\"] == \"item\":\n",
    "        entity = WikidataItem(entity_dict)\n",
    "        if ii< 15000000:\n",
    "            continue\n",
    "        try:\n",
    "            # 判断es中是否已经存在\n",
    "            if es.exists(index=index_name,id=entity.entity_id):\n",
    "                continue\n",
    "\n",
    "            # 判断是否存在英文wiki标题\n",
    "            en_wiki_title = entity.get_enwiki_title()\n",
    "            if not bool(en_wiki_title):\n",
    "                continue\n",
    "\n",
    "            # 判断是否存在英文标签\n",
    "            en_label = entity.get_label('en')\n",
    "            if not bool(en_label):\n",
    "                continue\n",
    "            # 判断是否存在英文描述\n",
    "            en_description = entity.get_description('en')\n",
    "            if not bool(en_description):\n",
    "                continue\n",
    "            \n",
    "            # 判断是否有P31\n",
    "            instance_list = entity.get_claim_group('P31')\n",
    "            if len(instance_list) <=0:\n",
    "                continue\n",
    "            \n",
    "            # 判断P31 是否在 list中\n",
    "            instance_value_list = [claim.mainsnak.datavalue.value[\"id\"] for claim in entity.get_truthy_claim_group('P31')]\n",
    "            if not bool(property_list & set(instance_value_list)):\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            # 捕获任意异常\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        item={\n",
    "            '_id': entity.entity_id,\n",
    "            '_index': index_name,\n",
    "            'entity_id':entity.entity_id,\n",
    "            'label':en_label,\n",
    "            'description':en_description,\n",
    "            'enwiki_title':en_wiki_title,\n",
    "            'instance_of': str(instance_value_list)\n",
    "        }\n",
    "        item_list.append(item)\n",
    "        # 每bs条插入\n",
    "        if len(item_list) > batch_size:\n",
    "            helpers.bulk(client=es, actions=item_list)\n",
    "            batch_num+=1\n",
    "            item_list=[]\n",
    "            print(f\"batch num:{batch_num} has been inserted.The data efficiency rate is {batch_num * batch_size / ii}\")\n",
    "\n",
    "# 插入余下的数据\n",
    "helpers.bulk(client=es, actions=item_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging to CSV: 100%|██████████| 1000000/1000000 [00:01<00:00, 760608.62lines/s]\n"
     ]
    }
   ],
   "source": [
    "# 构建CSV\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_to_csv(file1_path, file2_path, output_csv_path):\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1, open(file2_path, 'r', encoding='utf-8') as file2:\n",
    "        lines1 = file1.readlines()\n",
    "        lines2 = file2.readlines()\n",
    "        num_lines = len(lines1)\n",
    "    \n",
    "\n",
    "    # Use tqdm for progress visualization\n",
    "    with tqdm(total=num_lines, desc='Merging to CSV', unit='lines') as pbar:\n",
    "        # Combine lines from both files into a list of tuples\n",
    "        combined_lines = [(lines1[i].strip(), lines2[i].strip()) for i in range(num_lines)]\n",
    "        pbar.update(num_lines)\n",
    "\n",
    "    # Write the combined data to a CSV file\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['sent0', 'sent1'])  # Header\n",
    "        csv_writer.writerows(combined_lines)\n",
    "\n",
    "sent0_file = 'data/wiki1m_for_simcse.txt'\n",
    "sent1_file = 'data/wiki1m_for_simcse.txt'\n",
    "output_file = 'data/wiki1m_for_simcse_self_positive.csv'\n",
    "\n",
    "merge_to_csv(sent0_file, sent1_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging to CSV: 100%|██████████| 1000000/1000000 [00:01<00:00, 766361.89lines/s]\n"
     ]
    }
   ],
   "source": [
    "# 构建csv三元组\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_to_csv(file1_path, file2_path, file3_path, output_csv_path):\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1, open(file2_path, 'r', encoding='utf-8') as file2, open(file3_path, 'r', encoding='utf-8') as file3:\n",
    "        lines1 = file1.readlines()\n",
    "        lines2 = file2.readlines()\n",
    "        lines3 = file3.readlines()\n",
    "        num_lines = len(lines1)\n",
    "    \n",
    "\n",
    "    # Use tqdm for progress visualization\n",
    "    with tqdm(total=num_lines, desc='Merging to CSV', unit='lines') as pbar:\n",
    "        # Combine lines from both files into a list of tuples\n",
    "        combined_lines = [(lines1[i].strip(), lines2[i].strip(), lines3[i].strip()) for i in range(num_lines)]\n",
    "        pbar.update(num_lines)\n",
    "\n",
    "    # Write the combined data to a CSV file\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['sent0', 'sent1', 'sent2'])  # Header\n",
    "        csv_writer.writerows(combined_lines)\n",
    "\n",
    "sent0_file = 'data/wiki1m_for_simcse.txt'\n",
    "sent1_file = 'data/wiki1m_for_simcse.txt'\n",
    "sent2_file = 'data/wiki1m_for_simcse_person_description.txt'\n",
    "output_file = 'data/simcse_(origin_origin_person).csv'\n",
    "\n",
    "merge_to_csv(sent0_file, sent1_file, sent2_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机采样\n",
    "import random\n",
    "\n",
    "def sample_sentences(input_file, output_file, num_samples):\n",
    "    # 读取输入文件的所有句子\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    # 随机采样指定数量的句子\n",
    "    sampled_sentences = random.sample(sentences, num_samples)\n",
    "\n",
    "    # 将采样的句子写入输出文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sampled_sentences:\n",
    "            f.write(sentence)\n",
    "\n",
    "# 示例：从 input_file 中随机采样 1000 个句子到 output_file 中\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "output_file = 'data/wiki1k_for_debug.txt'\n",
    "num_samples = 1000\n",
    "sample_sentences(input_file, output_file, num_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
