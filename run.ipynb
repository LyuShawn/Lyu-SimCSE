{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试prompt-bert\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用 'bert-base-uncased' 模型\n",
    "model_name = 'result/28/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "# sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "sentence2 = template.format(sentence=sentence1)\n",
    "\n",
    "\n",
    "# 将句子转换为token ID，并添加特殊token [CLS] 和 [SEP]\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 模型不需要计算梯度，因此使用 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# BERT 输出的是一个包含多层的输出，这里我们只关心最后一层的隐藏状态\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "\n",
    "# 取 [CLS] token 对应的向量，作为整个句子的向量表示\n",
    "sentence_embedding1 = last_hidden_state1[:, 0, :]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = last_hidden_state2[:, 0, :]  # [batch_size, hidden_size]\n",
    "sentence2_embedding = last_hidden_state2[inputs2['input_ids'] == mask_token_id]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = sentence2_embedding.view(1, -1)\n",
    "\n",
    "cos_sim = F.cosine_similarity(sentence_embedding1, sentence2_embedding)\n",
    "\n",
    "print(cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "# 通过不同的方式计算句子表征\n",
    "# 1 什么都不做，直接使用 [CLS] token 的向量\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs)\n",
    "    emb1 = outputs1.last_hidden_state[:, 0, :]\n",
    "cos_sim = F.cosine_similarity(emb1, emb1)\n",
    "print(\"余弦相似度 (直接使用[CLS] token 向量):\", cos_sim.item())\n",
    "\n",
    "# 2 整个句子进去\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "prompt_sentenct = template.format(sentence=sentence).replace('[MASK]', tokenizer.mask_token)\n",
    "inputs2 = tokenizer(prompt_sentenct, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs2)\n",
    "    emb2 = outputs2.last_hidden_state[:, 0, :] # cls\n",
    "    emb3 = outputs2.last_hidden_state[inputs2['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim = F.cosine_similarity(emb1, emb2)\n",
    "cos_sim2 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"余弦相似度 (使用Prompt-BERT的整个句子向量):\", cos_sim.item())\n",
    "print(\"余弦相似度 (使用Prompt-BERT的[MASK] token 向量):\", cos_sim2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 引入attention mask\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'.replace('[MASK]', tokenizer.mask_token)\n",
    "prefix = template.split(\"{sentence}\")[0]\n",
    "suffix = template.split(\"{sentence}\")[1]\n",
    "prefix_input_ids = tokenizer(prefix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "suffix_inputs_ids = tokenizer(suffix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "sentence_inputs_ids = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "\n",
    "prefix_input_ids = prefix_input_ids[:-1]\n",
    "suffix_inputs_ids = suffix_inputs_ids[1:]\n",
    "sentence_inputs_ids = sentence_inputs_ids\n",
    "input_ids = torch.cat([prefix_input_ids, sentence_inputs_ids, suffix_inputs_ids])\n",
    "\n",
    "prompt_weight = 0\n",
    "attention_mask = torch.cat([\n",
    "    torch.full(prefix_input_ids.size(),prompt_weight),\n",
    "    torch.full(sentence_inputs_ids.size(),float(1)),\n",
    "    torch.full(suffix_inputs_ids.size(),prompt_weight)\n",
    "])\n",
    "\n",
    "inputs3 = {'input_ids': input_ids.view(1, -1), 'attention_mask': attention_mask.view(1, -1)}\n",
    "with torch.no_grad():\n",
    "    outputs3 = model(**inputs3)\n",
    "    emb3 = outputs3.last_hidden_state[inputs3['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim3 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"引入attention mask的的相似度\", cos_sim3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样数据集\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'data/wiki1m_for_simcse_ner.json'\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(\"数据集大小:\", len(dataset))\n",
    "print(\"数据集示例:\", dataset[0])\n",
    "\n",
    "# 采样1000个样本\n",
    "dataset_sample = random.sample(dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = 'data/wiki1m_for_simcse_ner_entity_dict.json'\n",
    "with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "    entity_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "output_path = 'data/wiki1k_with_entity_knowledge.json'\n",
    "for item in dataset_sample:\n",
    "    entity_list = item['entities']\n",
    "    print(item)\n",
    "    for entity in entity_list:\n",
    "        entity_hash = hash(entity['text']) + '.json'\n",
    "        if entity_hash in entity_dict:\n",
    "            entity['knowledge'] = entity_dict[entity_hash]\n",
    "            print(entity)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试apex是否可用\n",
    "from transformers.utils import is_apex_available\n",
    "print(is_apex_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评测能力\n",
    "from utils.auto_eval import eval\n",
    "\n",
    "eval('model/unsup-PromptBERT-baseline',pooler='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试bert attention mask\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "\n",
    "inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "\n",
    "padding = torch.full((inputs['input_ids'].shape[0], 5), fill_value=0, dtype=inputs['input_ids'].dtype)\n",
    "\n",
    "output = model(input_ids=padding)\n",
    "last_hidden_state1 = output.last_hidden_state[:, 0, :]\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2459/1000001 [02:21<15:58:40, 17.34it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 计算topk高的相似度的句子\u001b[39;00m\n\u001b[1;32m     56\u001b[0m topk_index \u001b[38;5;241m=\u001b[39m cos_sim\u001b[38;5;241m.\u001b[39mtopk(_topk)\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m---> 57\u001b[0m topk_sentences \u001b[38;5;241m=\u001b[39m [sentences[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m topk_index]\n\u001b[1;32m     58\u001b[0m key \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m text_encode(sent)\n\u001b[1;32m     59\u001b[0m info \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 计算topk高的相似度的句子\u001b[39;00m\n\u001b[1;32m     56\u001b[0m topk_index \u001b[38;5;241m=\u001b[39m cos_sim\u001b[38;5;241m.\u001b[39mtopk(_topk)\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m---> 57\u001b[0m topk_sentences \u001b[38;5;241m=\u001b[39m [sentences[\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m topk_index]\n\u001b[1;32m     58\u001b[0m key \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m text_encode(sent)\n\u001b[1;32m     59\u001b[0m info \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 检索测试\n",
    "from knowledge.retrieval import retrieve_knowledge\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import redis\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "r = redis.Redis(host='59.77.134.205', port=6379, db=2, password='lyuredis579')\n",
    "prefix = 'similarity_sent_'\n",
    "topk = 10\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "for sent in tqdm(sent_list):\n",
    "    \n",
    "    key = prefix + text_encode(sent)\n",
    "    if r.exists(key):\n",
    "        continue\n",
    "    summary = retrieve_knowledge(sent, retrieve_type=\"summary\")\n",
    "    if not summary:\n",
    "        continue\n",
    "    sentences = sent_tokenize(summary)\n",
    "    # 清洗\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if \"References\" in sentence:\n",
    "            sentences = sentences[:i]\n",
    "            break\n",
    "\n",
    "    _topk = topk if len(sentences) > topk else len(sentences)\n",
    "    sentences = [sent] + sentences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # 第一句是输入句子，后面的是检索到的知识，做相似度计算\n",
    "        cos_sim = F.cosine_similarity(embeddings[0:1], embeddings[1:], dim=1)\n",
    "\n",
    "        # 计算topk高的相似度的句子\n",
    "        topk_index = cos_sim.topk(_topk).indices\n",
    "        topk_sentences = [sentences[i.item()] for i in topk_index]\n",
    "        key = prefix + text_encode(sent)\n",
    "        info = []\n",
    "        for i, sentence in enumerate(topk_sentences):\n",
    "            info.append((cos_sim[i].item(),sentence))\n",
    "        r.set(key, json.dumps(info))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
