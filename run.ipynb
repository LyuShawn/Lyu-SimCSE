{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试prompt-bert\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用 'bert-base-uncased' 模型\n",
    "model_name = 'result/28/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "# sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "sentence2 = template.format(sentence=sentence1)\n",
    "\n",
    "\n",
    "# 将句子转换为token ID，并添加特殊token [CLS] 和 [SEP]\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 模型不需要计算梯度，因此使用 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# BERT 输出的是一个包含多层的输出，这里我们只关心最后一层的隐藏状态\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "\n",
    "# 取 [CLS] token 对应的向量，作为整个句子的向量表示\n",
    "sentence_embedding1 = last_hidden_state1[:, 0, :]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = last_hidden_state2[:, 0, :]  # [batch_size, hidden_size]\n",
    "sentence2_embedding = last_hidden_state2[inputs2['input_ids'] == mask_token_id]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = sentence2_embedding.view(1, -1)\n",
    "\n",
    "cos_sim = F.cosine_similarity(sentence_embedding1, sentence2_embedding)\n",
    "\n",
    "print(cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "# 通过不同的方式计算句子表征\n",
    "# 1 什么都不做，直接使用 [CLS] token 的向量\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs)\n",
    "    emb1 = outputs1.last_hidden_state[:, 0, :]\n",
    "cos_sim = F.cosine_similarity(emb1, emb1)\n",
    "print(\"余弦相似度 (直接使用[CLS] token 向量):\", cos_sim.item())\n",
    "\n",
    "# 2 整个句子进去\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "prompt_sentenct = template.format(sentence=sentence).replace('[MASK]', tokenizer.mask_token)\n",
    "inputs2 = tokenizer(prompt_sentenct, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs2)\n",
    "    emb2 = outputs2.last_hidden_state[:, 0, :] # cls\n",
    "    emb3 = outputs2.last_hidden_state[inputs2['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim = F.cosine_similarity(emb1, emb2)\n",
    "cos_sim2 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"余弦相似度 (使用Prompt-BERT的整个句子向量):\", cos_sim.item())\n",
    "print(\"余弦相似度 (使用Prompt-BERT的[MASK] token 向量):\", cos_sim2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 引入attention mask\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'.replace('[MASK]', tokenizer.mask_token)\n",
    "prefix = template.split(\"{sentence}\")[0]\n",
    "suffix = template.split(\"{sentence}\")[1]\n",
    "prefix_input_ids = tokenizer(prefix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "suffix_inputs_ids = tokenizer(suffix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "sentence_inputs_ids = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "\n",
    "prefix_input_ids = prefix_input_ids[:-1]\n",
    "suffix_inputs_ids = suffix_inputs_ids[1:]\n",
    "sentence_inputs_ids = sentence_inputs_ids\n",
    "input_ids = torch.cat([prefix_input_ids, sentence_inputs_ids, suffix_inputs_ids])\n",
    "\n",
    "prompt_weight = 0\n",
    "attention_mask = torch.cat([\n",
    "    torch.full(prefix_input_ids.size(),prompt_weight),\n",
    "    torch.full(sentence_inputs_ids.size(),float(1)),\n",
    "    torch.full(suffix_inputs_ids.size(),prompt_weight)\n",
    "])\n",
    "\n",
    "inputs3 = {'input_ids': input_ids.view(1, -1), 'attention_mask': attention_mask.view(1, -1)}\n",
    "with torch.no_grad():\n",
    "    outputs3 = model(**inputs3)\n",
    "    emb3 = outputs3.last_hidden_state[inputs3['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim3 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"引入attention mask的的相似度\", cos_sim3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样数据集\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'data/wiki1m_for_simcse_ner.json'\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(\"数据集大小:\", len(dataset))\n",
    "print(\"数据集示例:\", dataset[0])\n",
    "\n",
    "# 采样1000个样本\n",
    "dataset_sample = random.sample(dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = 'data/wiki1m_for_simcse_ner_entity_dict.json'\n",
    "with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "    entity_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "output_path = 'data/wiki1k_with_entity_knowledge.json'\n",
    "for item in dataset_sample:\n",
    "    entity_list = item['entities']\n",
    "    print(item)\n",
    "    for entity in entity_list:\n",
    "        entity_hash = hash(entity['text']) + '.json'\n",
    "        if entity_hash in entity_dict:\n",
    "            entity['knowledge'] = entity_dict[entity_hash]\n",
    "            print(entity)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试apex是否可用\n",
    "from transformers.utils import is_apex_available\n",
    "print(is_apex_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评测能力\n",
    "from utils.auto_eval import eval\n",
    "\n",
    "eval('model/unsup-PromptBERT-baseline',pooler='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试bert attention mask\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "\n",
    "inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "\n",
    "padding = torch.full((inputs['input_ids'].shape[0], 5), fill_value=0, dtype=inputs['input_ids'].dtype)\n",
    "\n",
    "output = model(input_ids=padding)\n",
    "last_hidden_state1 = output.last_hidden_state[:, 0, :]\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索\n",
    "from knowledge.retrieval import retrieve_knowledge\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import redis\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "r = redis.Redis(host='59.77.134.205', port=6379, db=2, password='lyuredis579')\n",
    "prefix = 'similarity_sent_'\n",
    "topk = 10\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "for sent in tqdm(sent_list):\n",
    "    \n",
    "    key = prefix + text_encode(sent)\n",
    "    if r.exists(key):\n",
    "        continue\n",
    "    summary = retrieve_knowledge(sent, retrieve_type=\"summary\")\n",
    "    if not summary:\n",
    "        continue\n",
    "    sentences = sent_tokenize(summary)\n",
    "    # 清洗\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if \"References\" in sentence:\n",
    "            sentences = sentences[:i]\n",
    "            break\n",
    "\n",
    "    _topk = topk if len(sentences) > topk else len(sentences)\n",
    "    sentences = [sent] + sentences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # 第一句是输入句子，后面的是检索到的知识，做相似度计算\n",
    "        cos_sim = F.cosine_similarity(embeddings[0:1], embeddings[1:], dim=1)\n",
    "\n",
    "        # 计算topk高的相似度的句子\n",
    "        topk_index = cos_sim.topk(_topk).indices\n",
    "        topk_sentences = [sentences[i.item()] for i in topk_index]\n",
    "        key = prefix + text_encode(sent)\n",
    "        info = []\n",
    "        for i, sentence in enumerate(topk_sentences):\n",
    "            info.append((cos_sim[i].item(),sentence))\n",
    "        r.set(key, json.dumps(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索\n",
    "from knowledge.retrieval import retrieve_knowledge\n",
    "import redis\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "\n",
    "prefix = 'similarity_sent_'\n",
    "topk = 10\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "sent_list = sent_list[:10]\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=2, password='lyuredis579')\n",
    "\n",
    "for sent in tqdm(sent_list):\n",
    "    value = retrieve_knowledge(sent, retrieve_type=\"sentence\")\n",
    "    for sim, sent in value:\n",
    "        print(sim)\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成新的数据集\n",
    "from knowledge.retrieval import retrieve_knowledge\n",
    "import base64\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for sent in tqdm(sent_list):\n",
    "    value = retrieve_knowledge(sent, retrieve_type=\"sentence\")\n",
    "    if value:\n",
    "        new_list.append((sent, value))\n",
    "\n",
    "bs = 128\n",
    "total = 1e6 # 100w\n",
    "batch_num = int(total / bs)\n",
    "print(\"新数据集大小:\", len(new_list))\n",
    "\n",
    "empty_rate = len(new_list) / len(sent_list)\n",
    "print(\"空值率:\", empty_rate)\n",
    "empty_in_batch = int(bs * empty_rate)\n",
    "add_num = int(bs - empty_in_batch)\n",
    "print(\"每个batch增加{}个样本\".format(add_num))\n",
    "\n",
    "output_file = 'data/wiki1m_for_simcse_test0.txt'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for i in range (batch_num):\n",
    "        batch = []\n",
    "        orgin_batch = new_list[i * empty_in_batch: (i + 1) * empty_in_batch]\n",
    "        # 从value中补充add_num个\n",
    "        new_sent = []\n",
    "        # 把所有的value组成一个list\n",
    "        value_list = []\n",
    "        for _, value in orgin_batch:\n",
    "            for sim,sent in value:\n",
    "                if sim > 0.3 and sim < 0.9 and \"==\" not in sent:\n",
    "                    value_list.append(sent)\n",
    "        value_list = list(set(value_list))\n",
    "        random.shuffle(value_list)\n",
    "        value_list = value_list[:add_num]\n",
    "        # 组成新的batch\n",
    "        batch = value_list + [sent for sent, _ in orgin_batch]\n",
    "        print(\"batch大小:\", len(batch))\n",
    "        f.write('\\n'.join(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "inputs = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "a = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查知识库命中率\n",
    "from knowledge.retrieval import retrieval_knowledge_batch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "bs = 64\n",
    "hit_rate_list = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in tqdm(range(0, len(sent_list), bs)):\n",
    "    sent_batch = sent_list[i:i+bs]\n",
    "    knowledge = retrieval_knowledge_batch(sent_batch, retrieve_type='title', title_num=-1)\n",
    "\n",
    "    title_list = knowledge\n",
    "    title_num_list = [len(t) for t in title_list]\n",
    "    count += sum([1 for num in title_num_list if num >= 1])\n",
    "    hit_count = sum(title_num_list)\n",
    "\n",
    "    # hit_count = sum([1 for k in knowledge if k])\n",
    "    hit_rate_list.append(hit_count / bs)\n",
    "\n",
    "print(f\"多知识:{count}\")\n",
    "\n",
    "mean = np.mean(hit_rate_list)\n",
    "median = np.median(hit_rate_list)\n",
    "std_dev = np.std(hit_rate_list)\n",
    "min_val = np.min(hit_rate_list)\n",
    "max_val = np.max(hit_rate_list)\n",
    "percentiles = np.perxcentile(hit_rate_list, [25, 50, 75])\n",
    "\n",
    "print(f\"平均值: {mean}, 中位数: {median}, 标准差: {std_dev}, 最小值: {min_val}, 最大值: {max_val}, 四分位数: {percentiles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 检查知识库命中率\n",
    "from knowledge.retrieval import retrieval_knowledge_batch\n",
    "from tqdm import tqdm\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "# sent_list = sent_list[:1000]\n",
    "\n",
    "knowledge_list = retrieval_knowledge_batch(sent_list, retrieve_type='sentence')\n",
    "\n",
    "cnt = 0\n",
    "for knowledge in tqdm(knowledge_list):\n",
    "    cnt += len(knowledge)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_delta(template_token, length=50):\n",
    "    with torch.set_grad_enabled(not cls.model_args.mask_embedding_sentence_delta_freeze):\n",
    "        device = input_ids.device\n",
    "        # length, template_token len\n",
    "        d_input_ids = torch.Tensor(template_token).repeat(length, 1).to(device).long()\n",
    "        d_inputs_embeds = None\n",
    "\n",
    "        d_position_ids = torch.arange(d_input_ids.shape[1]).to(device).unsqueeze(0).repeat(length, 1).long()\n",
    "            \n",
    "        d_position_ids[:, len(cls.bs)+1:] += torch.arange(length).to(device).unsqueeze(-1)\n",
    "        \n",
    "        m_mask = d_input_ids == cls.mask_token_id\n",
    "        outputs = encoder(input_ids=d_input_ids if d_inputs_embeds is None else None ,\n",
    "                            inputs_embeds=d_inputs_embeds,\n",
    "                            position_ids=d_position_ids,  output_hidden_states=True, return_dict=True)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        delta = last_hidden[m_mask]\n",
    "        template_len = d_input_ids.shape[1]\n",
    "        if cls.model_args.mask_embedding_sentence_org_mlp:\n",
    "            delta = cls.mlp(delta)\n",
    "        return delta, template_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 test\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 加载 T5 模型和分词器\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 准备输入文本\n",
    "context = \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from and make decisions based on data. Unlike traditional programming, machine learning models improve over time with more data.\"\n",
    "question = \"What is the focus of machine learning?\"\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "# 对输入文本进行分词并转换为张量\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 使用模型生成输出\n",
    "output_ids = model.generate(inputs.input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# 解码生成的输出\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Generated Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge.retrieval import retrieval_knowledge\n",
    "\n",
    "from tqdm import tqdm\n",
    "input_file = 'data/wiki1m_for_simcse.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    sent_list = f.read().splitlines()\n",
    "\n",
    "sent_list = sent_list[:1000]\n",
    "\n",
    "knowledge_list = retrieval_knowledge(sent_list, retrieve_type='sentence')\n",
    "\n",
    "pass\n",
    "\n",
    "cnt = 0\n",
    "sent_cnt = 0\n",
    "for knowledge in tqdm(knowledge_list):\n",
    "    o_cnt = cnt\n",
    "    for sim, sent in knowledge:\n",
    "        if sim > 0.6:\n",
    "            cnt += 1\n",
    "    if cnt > o_cnt:\n",
    "        sent_cnt += 1\n",
    "print(cnt)\n",
    "print(sent_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1 and sent2: 0.7866969704627991\n",
      "sent1 and k_sent1: 0.8518177270889282\n",
      "sent2 and k_sent1: 0.6989714503288269\n"
     ]
    }
   ],
   "source": [
    "# 测试prompt的相似度\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "model_name = \"princeton-nlp/unsup-simcse-bert-base-uncased\"\n",
    "# model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "s1 = \"Deep learning models have transformed image recognition.\"\n",
    "s2 = \"Convolutional neural networks are commonly used for image classification.\"\n",
    "\n",
    "# 句子1的外部知识\n",
    "knowledge_sent1 = [\n",
    "    \"Deep learning is a subset of machine learning that uses neural networks with many layers.\",\n",
    "    \"Image recognition involves identifying objects, people, or features in an image.\",\n",
    "    \"Convolutional neural networks (CNNs) are a type of deep learning model particularly effective for image data.\"\n",
    "]\n",
    "\n",
    "prompt1 = \"This sentence of '{sentence}' means [MASK].\".replace('[MASK]', tokenizer.mask_token)\n",
    "prompt2 = \"The phrase '{sentence}' may relate to '{knowledge}', thus [MASK] is implied.\".replace('[MASK]', tokenizer.mask_token)\n",
    "# prompt2 = prompt\n",
    "\n",
    "sent1 = prompt1.format(sentence=s1)\n",
    "sent2 = prompt1.format(sentence=s2)\n",
    "k_sent1 = prompt2.format(sentence=s1, knowledge=knowledge_sent1[1])\n",
    "\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "inputs1 = tokenizer(sent1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sent2, return_tensors='pt', padding=True, truncation=True)\n",
    "k_inputs1 = tokenizer(k_sent1, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = model(**inputs1)\n",
    "    output2 = model(**inputs2)\n",
    "\n",
    "    k_output1 = model(**k_inputs1)\n",
    "\n",
    "emb1 = output1.last_hidden_state[inputs1[\"input_ids\"] == mask_token_id]\n",
    "emb2 = output2.last_hidden_state[inputs2[\"input_ids\"] == mask_token_id]\n",
    "k_emb1 = k_output1.last_hidden_state[k_inputs1[\"input_ids\"] == mask_token_id]\n",
    "\n",
    "print(f\"sent1 and sent2: {F.cosine_similarity(emb1, emb2).item()}\")\n",
    "print(f\"sent1 and k_sent1: {F.cosine_similarity(emb1, k_emb1).item()}\")\n",
    "print(f\"sent2 and k_sent1: {F.cosine_similarity(emb2, k_emb1).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (958152846.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    template = \"This sentence of \"{sentence}\" means [MASK].\"\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "template = \"*cls*_This_sentence_of_\\\"*sent_0*\\\"_means*mask*.*sep+*\"\n",
    "\n",
    "template = template.replace('*mask*', tokenizer.mask_token)\\\n",
    "                    .replace('*sep+*', '')\\\n",
    "                    .replace('*cls*', '').replace('*sent_0*', ' ')\n",
    "template = template.split(' ')\n",
    "\n",
    "print(template)\n",
    "\n",
    "template = \"The phrase \\\"{sentence}\\\" may relate to \\\"{knowledge}\\\", thus [MASK] is implied.\"\n",
    "template = \"This sentence of \\\"{sentence}\\\" means [MASK].\"\n",
    "template = template.split(\"{sentence}\")\n",
    "print(template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
