{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试prompt-bert\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用 'bert-base-uncased' 模型\n",
    "model_name = 'result/28/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "# sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "sentence2 = template.format(sentence=sentence1)\n",
    "\n",
    "\n",
    "# 将句子转换为token ID，并添加特殊token [CLS] 和 [SEP]\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 模型不需要计算梯度，因此使用 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# BERT 输出的是一个包含多层的输出，这里我们只关心最后一层的隐藏状态\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "\n",
    "# 取 [CLS] token 对应的向量，作为整个句子的向量表示\n",
    "sentence_embedding1 = last_hidden_state1[:, 0, :]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = last_hidden_state2[:, 0, :]  # [batch_size, hidden_size]\n",
    "sentence2_embedding = last_hidden_state2[inputs2['input_ids'] == mask_token_id]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = sentence2_embedding.view(1, -1)\n",
    "\n",
    "cos_sim = F.cosine_similarity(sentence_embedding1, sentence2_embedding)\n",
    "\n",
    "print(cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "余弦相似度 (直接使用[CLS] token 向量): 0.9999999403953552\n",
      "余弦相似度 (使用Prompt-BERT的整个句子向量): 0.855078935623169\n",
      "余弦相似度 (使用Prompt-BERT的[MASK] token 向量): 0.8600753545761108\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "# 通过不同的方式计算句子表征\n",
    "# 1 什么都不做，直接使用 [CLS] token 的向量\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs)\n",
    "    emb1 = outputs1.last_hidden_state[:, 0, :]\n",
    "cos_sim = F.cosine_similarity(emb1, emb1)\n",
    "print(\"余弦相似度 (直接使用[CLS] token 向量):\", cos_sim.item())\n",
    "\n",
    "# 2 整个句子进去\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "prompt_sentenct = template.format(sentence=sentence).replace('[MASK]', tokenizer.mask_token)\n",
    "inputs2 = tokenizer(prompt_sentenct, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs2)\n",
    "    emb2 = outputs2.last_hidden_state[:, 0, :] # cls\n",
    "    emb3 = outputs2.last_hidden_state[inputs2['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim = F.cosine_similarity(emb1, emb2)\n",
    "cos_sim2 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"余弦相似度 (使用Prompt-BERT的整个句子向量):\", cos_sim.item())\n",
    "print(\"余弦相似度 (使用Prompt-BERT的[MASK] token 向量):\", cos_sim2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "引入attention mask的的相似度 0.9417366981506348\n"
     ]
    }
   ],
   "source": [
    "# 3 引入attention mask\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'.replace('[MASK]', tokenizer.mask_token)\n",
    "prefix = template.split(\"{sentence}\")[0]\n",
    "suffix = template.split(\"{sentence}\")[1]\n",
    "prefix_input_ids = tokenizer(prefix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "suffix_inputs_ids = tokenizer(suffix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "sentence_inputs_ids = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "\n",
    "prefix_input_ids = prefix_input_ids[:-1]\n",
    "suffix_inputs_ids = suffix_inputs_ids[1:]\n",
    "sentence_inputs_ids = sentence_inputs_ids\n",
    "input_ids = torch.cat([prefix_input_ids, sentence_inputs_ids, suffix_inputs_ids])\n",
    "\n",
    "prompt_weight = 0\n",
    "attention_mask = torch.cat([\n",
    "    torch.full(prefix_input_ids.size(),prompt_weight),\n",
    "    torch.full(sentence_inputs_ids.size(),float(1)),\n",
    "    torch.full(suffix_inputs_ids.size(),prompt_weight)\n",
    "])\n",
    "\n",
    "inputs3 = {'input_ids': input_ids.view(1, -1), 'attention_mask': attention_mask.view(1, -1)}\n",
    "with torch.no_grad():\n",
    "    outputs3 = model(**inputs3)\n",
    "    emb3 = outputs3.last_hidden_state[inputs3['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim3 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"引入attention mask的的相似度\", cos_sim3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试apex是否可用\n",
    "from transformers.utils import is_apex_available\n",
    "print(is_apex_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评测能力\n",
    "from utils.auto_eval import eval\n",
    "\n",
    "eval('model/unsup-PromptBERT-baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
