{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试prompt-bert\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用 'bert-base-uncased' 模型\n",
    "model_name = 'result/28/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "# sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "sentence2 = template.format(sentence=sentence1)\n",
    "\n",
    "\n",
    "# 将句子转换为token ID，并添加特殊token [CLS] 和 [SEP]\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 模型不需要计算梯度，因此使用 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# BERT 输出的是一个包含多层的输出，这里我们只关心最后一层的隐藏状态\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "\n",
    "# 取 [CLS] token 对应的向量，作为整个句子的向量表示\n",
    "sentence_embedding1 = last_hidden_state1[:, 0, :]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = last_hidden_state2[:, 0, :]  # [batch_size, hidden_size]\n",
    "sentence2_embedding = last_hidden_state2[inputs2['input_ids'] == mask_token_id]  # [batch_size, hidden_size]\n",
    "# sentence2_embedding = sentence2_embedding.view(1, -1)\n",
    "\n",
    "cos_sim = F.cosine_similarity(sentence_embedding1, sentence2_embedding)\n",
    "\n",
    "print(cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "# 通过不同的方式计算句子表征\n",
    "# 1 什么都不做，直接使用 [CLS] token 的向量\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs)\n",
    "    emb1 = outputs1.last_hidden_state[:, 0, :]\n",
    "cos_sim = F.cosine_similarity(emb1, emb1)\n",
    "print(\"余弦相似度 (直接使用[CLS] token 向量):\", cos_sim.item())\n",
    "\n",
    "# 2 整个句子进去\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'\n",
    "prompt_sentenct = template.format(sentence=sentence).replace('[MASK]', tokenizer.mask_token)\n",
    "inputs2 = tokenizer(prompt_sentenct, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs2)\n",
    "    emb2 = outputs2.last_hidden_state[:, 0, :] # cls\n",
    "    emb3 = outputs2.last_hidden_state[inputs2['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim = F.cosine_similarity(emb1, emb2)\n",
    "cos_sim2 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"余弦相似度 (使用Prompt-BERT的整个句子向量):\", cos_sim.item())\n",
    "print(\"余弦相似度 (使用Prompt-BERT的[MASK] token 向量):\", cos_sim2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 引入attention mask\n",
    "template = 'This sentence : \"{sentence}\" means [MASK].'.replace('[MASK]', tokenizer.mask_token)\n",
    "prefix = template.split(\"{sentence}\")[0]\n",
    "suffix = template.split(\"{sentence}\")[1]\n",
    "prefix_input_ids = tokenizer(prefix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "suffix_inputs_ids = tokenizer(suffix, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "sentence_inputs_ids = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)['input_ids'].view(-1)\n",
    "\n",
    "prefix_input_ids = prefix_input_ids[:-1]\n",
    "suffix_inputs_ids = suffix_inputs_ids[1:]\n",
    "sentence_inputs_ids = sentence_inputs_ids\n",
    "input_ids = torch.cat([prefix_input_ids, sentence_inputs_ids, suffix_inputs_ids])\n",
    "\n",
    "prompt_weight = 0\n",
    "attention_mask = torch.cat([\n",
    "    torch.full(prefix_input_ids.size(),prompt_weight),\n",
    "    torch.full(sentence_inputs_ids.size(),float(1)),\n",
    "    torch.full(suffix_inputs_ids.size(),prompt_weight)\n",
    "])\n",
    "\n",
    "inputs3 = {'input_ids': input_ids.view(1, -1), 'attention_mask': attention_mask.view(1, -1)}\n",
    "with torch.no_grad():\n",
    "    outputs3 = model(**inputs3)\n",
    "    emb3 = outputs3.last_hidden_state[inputs3['input_ids'] == tokenizer.mask_token_id]  # mask\n",
    "cos_sim3 = F.cosine_similarity(emb1, emb3)\n",
    "print(\"引入attention mask的的相似度\", cos_sim3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样数据集\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'data/wiki1m_for_simcse_ner.json'\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(\"数据集大小:\", len(dataset))\n",
    "print(\"数据集示例:\", dataset[0])\n",
    "\n",
    "# 采样1000个样本\n",
    "dataset_sample = random.sample(dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = 'data/wiki1m_for_simcse_ner_entity_dict.json'\n",
    "with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "    entity_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "output_path = 'data/wiki1k_with_entity_knowledge.json'\n",
    "for item in dataset_sample:\n",
    "    entity_list = item['entities']\n",
    "    print(item)\n",
    "    for entity in entity_list:\n",
    "        entity_hash = hash(entity['text']) + '.json'\n",
    "        if entity_hash in entity_dict:\n",
    "            entity['knowledge'] = entity_dict[entity_hash]\n",
    "            print(entity)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 测试apex是否可用\n",
    "from transformers.utils import is_apex_available\n",
    "print(is_apex_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评测能力\n",
    "from utils.auto_eval import eval\n",
    "\n",
    "eval('model/unsup-PromptBERT-baseline',pooler='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "# 测试bert attention mask\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"I love machine learning and natural language processing.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "\n",
    "inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "\n",
    "padding = torch.full((inputs['input_ids'].shape[0], 5), fill_value=0, dtype=inputs['input_ids'].dtype)\n",
    "\n",
    "output = model(input_ids=padding)\n",
    "last_hidden_state1 = output.last_hidden_state[:, 0, :]\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YMCA in South Australia,Our Boys Institute,Jack Massey (footballer)\n"
     ]
    }
   ],
   "source": [
    "# 检索测试\n",
    "from knowledge.retrieval import retrieve_knowledge\n",
    "\n",
    "sent = 'YMCA in South Australia'\n",
    "result = retrieve_knowledge(sent)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
