{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评测能力\n",
    "from eval import eval\n",
    "\n",
    "eval('result/40/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试prompt-bert\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用 'bert-base-uncased' 模型\n",
    "model_name = '/root/project/Prompt-BERT/result/unsup-bert_s42/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "\n",
    "# 将句子转换为token ID，并添加特殊token [CLS] 和 [SEP]\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 模型不需要计算梯度，因此使用 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# BERT 输出的是一个包含多层的输出，这里我们只关心最后一层的隐藏状态\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "\n",
    "# 取 [CLS] token 对应的向量，作为整个句子的向量表示\n",
    "sentence_embedding1 = last_hidden_state1[:, 0, :]  # [batch_size, hidden_size]\n",
    "sentence2_embedding = last_hidden_state2[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "cos_sim = F.cosine_similarity(sentence_embedding1, sentence2_embedding)\n",
    "\n",
    "print(cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用Prompt-BERT预训练模型\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 定义带有 [MASK] 的prompt模板\n",
    "def create_prompt_with_mask(sentence):\n",
    "    # 在句子中插入 [MASK]，这里可以根据需求灵活调整位置\n",
    "    prompt_template = f\"The meaning of the sentence: {sentence} is [MASK].\"\n",
    "    return prompt_template\n",
    "\n",
    "# 示例句子\n",
    "sentence1 = \"I love machine learning and natural language processing.\"\n",
    "sentence2 = \"I hate machine learning and natural language processing.\"\n",
    "\n",
    "# 为句子添加 [MASK] prompt\n",
    "prompted_sentence1 = create_prompt_with_mask(sentence1)\n",
    "prompted_sentence2 = create_prompt_with_mask(sentence2)\n",
    "\n",
    "# 将句子转换为token ID\n",
    "inputs1 = tokenizer(prompted_sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(prompted_sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# 获取 [MASK] token 的位置\n",
    "mask_token_index1 = (inputs1['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "mask_token_index2 = (inputs2['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "# 不需要计算梯度\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# 提取 [MASK] token 的向量表示\n",
    "mask_embedding1 = outputs1.last_hidden_state[0, mask_token_index1, :].squeeze(0)\n",
    "mask_embedding2 = outputs2.last_hidden_state[0, mask_token_index2, :].squeeze(0)\n",
    "\n",
    "# 确保向量维度正确\n",
    "mask_embedding1 = mask_embedding1.unsqueeze(0)  # 转为 [1, hidden_size] 形式\n",
    "mask_embedding2 = mask_embedding2.unsqueeze(0)\n",
    "\n",
    "# 计算两个句子 [MASK] token 向量的余弦相似度\n",
    "cos_sim = F.cosine_similarity(mask_embedding1, mask_embedding2, dim=1)\n",
    "\n",
    "print(\"余弦相似度 (使用Prompt-BERT的[MASK] token 向量):\", cos_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simcse import SimCSE\n",
    "model = SimCSE(\"result//\")\n",
    "\n",
    "sentences_a = ['I love machine learning and natural language processing.']\n",
    "sentences_b = ['I hate machine learning and natural language processing.']\n",
    "similarities = model.similarity(sentences_a, sentences_b)\n",
    "print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
