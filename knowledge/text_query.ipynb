{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于对语料库及wiki数据库进行查询进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算语料库的平均长度\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "file_path = 'data/wiki1m_knowledge.txt'\n",
    "\n",
    "# 读取文本文件，并计算每个句子的长度\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentence_word_counts = [len(line.strip().split()) for line in file]\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "sentence_word_counts_array = np.array(sentence_word_counts)\n",
    "\n",
    "# 计算中位数\n",
    "median = np.median(sentence_word_counts_array)\n",
    "\n",
    "# 计算众数\n",
    "mode = int(np.argmax(np.bincount(sentence_word_counts_array)))\n",
    "\n",
    "# 计算平均数\n",
    "mean = np.mean(sentence_word_counts_array)\n",
    "\n",
    "# 计算最大值和最小值\n",
    "max_value = np.max(sentence_word_counts_array)\n",
    "min_value = np.min(sentence_word_counts_array)\n",
    "\n",
    "print(\"中位数:\", median)\n",
    "print(\"众数:\", mode)\n",
    "print(\"平均数:\", mean)\n",
    "print(\"最大值:\", max_value)\n",
    "print(\"最小值:\", min_value)\n",
    "\n",
    "# 画频率分布图\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(sentence_word_counts, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert prompt test\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "path = '/pretrain_model/bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(path)\n",
    "model = BertModel.from_pretrained(path)\n",
    "text = \"Example sentence to be tokenized.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import os\n",
    "\n",
    "# 下载并加载BERT模型\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 将模型移动到指定的目录\n",
    "output_dir = 'pretrain_model/bert-base-uncased'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import json\n",
    "import flair\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "flair.device = torch.device('cuda')\n",
    "# 加载NER模型\n",
    "tagger = SequenceTagger.load(\"ner\")\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse.txt'\n",
    "output_file = '../data/wiki1m_for_simcse_ner.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(0, len(lines), batch_size)):\n",
    "    sentence_list = []\n",
    "    for line in lines[i:i+batch_size]:\n",
    "        sentence_list.append(Sentence(line))\n",
    "\n",
    "    tagger.predict(sentence_list, mini_batch_size=128, verbose=False)\n",
    "\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        entities_list = []\n",
    "        for i, entity in enumerate(sentence.get_spans('ner')):\n",
    "            entities_list.append({\n",
    "                \"text\": entity.text,\n",
    "                \"start_position\": entity.start_position,\n",
    "                \"end_position\": entity.end_position,\n",
    "                \"label\": entity.get_label('ner').value, \n",
    "                \"confidence\": entity.score\n",
    "            })\n",
    "        result.append({\"text\": sentence.to_original_text(), \"entities\": entities_list})\n",
    "\n",
    "# 保存结果\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # 2024-10-11 10:52:22,912 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据梳理\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse_ner.json'\n",
    "output_file = '../data/wiki1m_for_simcse_ner_entity.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    sentence_list = json.load(file)\n",
    "\n",
    "entity_list = []\n",
    "for sentence in sentence_list:\n",
    "    for entity in sentence['entities']:\n",
    "        entity_list.append(entity)\n",
    "\n",
    "print(\"实体数量:\", len(entity_list))\n",
    "\n",
    "# 去重\n",
    "entity_set = set()\n",
    "for entity in entity_list:\n",
    "    entity_set.add(entity['text'])\n",
    "\n",
    "print(\"去重后的实体数量:\", len(entity_set))\n",
    "# 实体数量: 1977083\n",
    "# 去重后的实体数量: 618928\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for entity in entity_set:\n",
    "        file.write(entity + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "base_dir = '../data/'\n",
    "\n",
    "input_file = base_dir + 'wiki1m_for_simcse_ner_entity.txt'\n",
    "output_dir = base_dir + 'wiki1m_for_simcse_ner_entity_search/'\n",
    "output_file = base_dir + 'wiki1m_for_simcse_ner_entity_search_dict.json'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    entity_list = file.read().splitlines()\n",
    "\n",
    "entity_dict = {}\n",
    "\n",
    "# 设置 API 的基础 URL\n",
    "wikidata_url = 'https://www.wikidata.org/w/api.php'\n",
    "\n",
    "def search_wikidata(entity):\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',  # 使用实体搜索\n",
    "        'format': 'json',              # 返回格式为JSON\n",
    "        'language': 'en',              # 查询语言\n",
    "        'search': entity,              # 要查询的实体名称\n",
    "        'limit': 10                    # 限制返回结果数量\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(wikidata_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('search', [])\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {entity}: {e}\")\n",
    "        return None\n",
    "\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "def process_entity(entity):\n",
    "    \"\"\"\n",
    "    查询实体并保存结果到文件\n",
    "    \"\"\"\n",
    "    output_file_path = output_dir + hash(entity) + '.json'\n",
    "    if os.path.exists(output_file_path):\n",
    "        return None\n",
    "    \n",
    "    # 查询实体\n",
    "    entities = search_wikidata(entity)\n",
    "    if entities is not None:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(entities, file, ensure_ascii=False, indent=4)\n",
    "    return entities\n",
    "\n",
    "# 使用线程池进行并发请求\n",
    "max_workers = 10  # 设置并发线程数量\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_entity, entity) for entity in entity_list]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        future.result()  # 等待每个任务完成\n",
    "\n",
    "# 都结束后，将所有实体信息整合到一个文件中\n",
    "entity_dict = {}\n",
    "for file in os.listdir(output_dir):\n",
    "    with open(output_dir + file, 'r', encoding='utf-8') as f:\n",
    "        entity_dict[file.split('.')[0]] = json.load(f)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(entity_dict, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
