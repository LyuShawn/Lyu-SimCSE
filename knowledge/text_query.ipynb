{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于对语料库及wiki数据库进行查询进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算语料库的平均长度\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "file_path = 'data/wiki1m_knowledge.txt'\n",
    "\n",
    "# 读取文本文件，并计算每个句子的长度\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentence_word_counts = [len(line.strip().split()) for line in file]\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "sentence_word_counts_array = np.array(sentence_word_counts)\n",
    "\n",
    "# 计算中位数\n",
    "median = np.median(sentence_word_counts_array)\n",
    "\n",
    "# 计算众数\n",
    "mode = int(np.argmax(np.bincount(sentence_word_counts_array)))\n",
    "\n",
    "# 计算平均数\n",
    "mean = np.mean(sentence_word_counts_array)\n",
    "\n",
    "# 计算最大值和最小值\n",
    "max_value = np.max(sentence_word_counts_array)\n",
    "min_value = np.min(sentence_word_counts_array)\n",
    "\n",
    "print(\"中位数:\", median)\n",
    "print(\"众数:\", mode)\n",
    "print(\"平均数:\", mean)\n",
    "print(\"最大值:\", max_value)\n",
    "print(\"最小值:\", min_value)\n",
    "\n",
    "# 画频率分布图\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(sentence_word_counts, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert prompt test\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "path = '/pretrain_model/bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(path)\n",
    "model = BertModel.from_pretrained(path)\n",
    "text = \"Example sentence to be tokenized.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import os\n",
    "\n",
    "# 下载并加载BERT模型\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 将模型移动到指定的目录\n",
    "output_dir = 'pretrain_model/bert-base-uncased'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import json\n",
    "import flair\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "flair.device = torch.device('cuda')\n",
    "# 加载NER模型\n",
    "tagger = SequenceTagger.load(\"ner\")\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse.txt'\n",
    "output_file = '../data/wiki1m_for_simcse_ner.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(0, len(lines), batch_size)):\n",
    "    sentence_list = []\n",
    "    for line in lines[i:i+batch_size]:\n",
    "        sentence_list.append(Sentence(line))\n",
    "\n",
    "    tagger.predict(sentence_list, mini_batch_size=128, verbose=False)\n",
    "\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        entities_list = []\n",
    "        for i, entity in enumerate(sentence.get_spans('ner')):\n",
    "            entities_list.append({\n",
    "                \"text\": entity.text,\n",
    "                \"start_position\": entity.start_position,\n",
    "                \"end_position\": entity.end_position,\n",
    "                \"label\": entity.get_label('ner').value, \n",
    "                \"confidence\": entity.score\n",
    "            })\n",
    "        result.append({\"text\": sentence.to_original_text(), \"entities\": entities_list})\n",
    "\n",
    "# 保存结果\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # 2024-10-11 10:52:22,912 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据梳理\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse_ner.json'\n",
    "output_file = '../data/wiki1m_for_simcse_ner_entity.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    sentence_list = json.load(file)\n",
    "\n",
    "entity_list = []\n",
    "for sentence in sentence_list:\n",
    "    for entity in sentence['entities']:\n",
    "        entity_list.append(entity)\n",
    "\n",
    "print(\"实体数量:\", len(entity_list))\n",
    "\n",
    "# 去重\n",
    "entity_set = set()\n",
    "for entity in entity_list:\n",
    "    entity_set.add(entity['text'])\n",
    "\n",
    "print(\"去重后的实体数量:\", len(entity_set))\n",
    "# 实体数量: 1977083\n",
    "# 去重后的实体数量: 618928\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for entity in entity_set:\n",
    "        file.write(entity + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "base_dir = '../data/'\n",
    "\n",
    "input_file = base_dir + 'wiki1m_for_simcse_ner_entity.txt'\n",
    "output_dir = base_dir + 'wiki1m_for_simcse_ner_entity_search_all/'\n",
    "output_file = base_dir + 'wiki1m_for_simcse_ner_entity_search_dict.json'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    entity_list = file.read().splitlines()\n",
    "\n",
    "entity_dict = {}\n",
    "\n",
    "# 设置 API 的基础 URL\n",
    "wikidata_url = 'https://www.wikidata.org/w/api.php'\n",
    "\n",
    "def search_wikidata(entity):\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',  # 使用实体搜索\n",
    "        'format': 'json',              # 返回格式为JSON\n",
    "        'language': 'en',              # 查询语言\n",
    "        'search': entity,              # 要查询的实体名称\n",
    "        'limit': 10                    # 限制返回结果数量\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(wikidata_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('search', [])\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {entity}: {e}\")\n",
    "        return None\n",
    "\n",
    "def hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "def process_entity(entity):\n",
    "    \"\"\"\n",
    "    查询实体并保存结果到文件\n",
    "    \"\"\"\n",
    "    output_file_path = output_dir + hash(entity) + '.json'\n",
    "    if os.path.exists(output_file_path):\n",
    "        return None\n",
    "    \n",
    "    # 查询实体\n",
    "    entities = search_wikidata(entity)\n",
    "    if entities is not None:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(entities, file, ensure_ascii=False, indent=4)\n",
    "    return entities\n",
    "\n",
    "# 使用线程池进行并发请求\n",
    "max_workers = 10  # 设置并发线程数量\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_entity, entity) for entity in entity_list]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        future.result()  # 等待每个任务完成\n",
    "\n",
    "# 都结束后，将所有实体信息整合到一个文件中\n",
    "entity_dict = {}\n",
    "for file in os.listdir(output_dir):\n",
    "    with open(output_dir + file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            entity_dict[file.split('.')[0]] = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(entity_dict, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = '../data/'\n",
    "input_dir = base_dir + 'wiki1m_for_simcse_ner_entity_search/'\n",
    "output_file = base_dir + 'wiki1m_for_simcse_ner_entity_dict.json'\n",
    "\n",
    "entity_dict = {}\n",
    "for file in tqdm(os.listdir(input_dir)):\n",
    "    with open(input_dir + file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            entity_dict[file.split('.')[0]] = json.load(f)\n",
    "        except:\n",
    "            print(file)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(entity_dict, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_entities(entity_name):\n",
    "    \"\"\"\n",
    "    搜索 Wikipedia 上的同名实体并返回候选列表\n",
    "    :param entity_name: 实体名称\n",
    "    :return: 同名实体的候选列表，每个项包含页面 ID 和标题\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": entity_name,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    search_results = response.json().get(\"query\", {}).get(\"search\", [])\n",
    "    \n",
    "    candidates = [{\"page_id\": result[\"pageid\"], \"title\": result[\"title\"]} for result in search_results]\n",
    "    return candidates\n",
    "\n",
    "def fetch_entity_info(page_id):\n",
    "    \"\"\"\n",
    "    根据页面 ID 获取 Wikipedia 实体的详细信息\n",
    "    :param page_id: Wikipedia 页面 ID\n",
    "    :return: 包含实体详细信息的字典\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"pageids\": page_id,\n",
    "        \"prop\": \"info|extracts|categories|links\",\n",
    "        \"inprop\": \"url\",\n",
    "        \"exintro\": True,\n",
    "        \"explaintext\": True,\n",
    "        \"cllimit\": \"max\",\n",
    "        \"pllimit\": \"max\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    page_info = response.json().get(\"query\", {}).get(\"pages\", {}).get(str(page_id), {})\n",
    "\n",
    "    entity_info = {\n",
    "        \"page_id\": page_id,\n",
    "        \"title\": page_info.get(\"title\", \"N/A\"),\n",
    "        \"url\": page_info.get(\"fullurl\", \"N/A\"),\n",
    "        \"extract\": page_info.get(\"extract\", \"No summary available\"),\n",
    "        \"categories\": [cat.get(\"title\", \"\") for cat in page_info.get(\"categories\", [])],\n",
    "        \"links\": [link.get(\"title\", \"\") for link in page_info.get(\"links\", [])]\n",
    "    }\n",
    "    \n",
    "    return entity_info\n",
    "\n",
    "def fetch_all_entity_infos(entity_name):\n",
    "    # 搜索同名实体的候选项\n",
    "    candidates = search_entities(entity_name)\n",
    "    if not candidates:\n",
    "        print(f\"No results found for '{entity_name}'\")\n",
    "        return\n",
    "    \n",
    "    # 获取所有候选实体的详细信息\n",
    "    all_entity_infos = []\n",
    "    for candidate in candidates:\n",
    "        print(f\"Fetching info for '{candidate['title']}' (Page ID: {candidate['page_id']})...\")\n",
    "        entity_info = fetch_entity_info(candidate[\"page_id\"])\n",
    "        all_entity_infos.append(entity_info)\n",
    "    \n",
    "    # 输出每个实体的信息\n",
    "    for info in all_entity_infos:\n",
    "        print(\"\\n页面标题:\", info['title'])\n",
    "        print(\"页面 URL:\", info['url'])\n",
    "        print(\"简介:\", info['extract'])\n",
    "        print('---')\n",
    "\n",
    "# 示例使用\n",
    "entity_name = \"Winner advances to the second stage.\"\n",
    "fetch_all_entity_infos(entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对假负样例进行分析\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sent_list = [\"Chamod Wickramasuriya\",\n",
    "    \"Chamod Wickramasuriya (born 27 May 1999) is a Sri Lankan cricketer.\",\n",
    "    \"He made his Twenty20 debut on 15 January 2020, for Galle Cricket Club in the 2019–20 SLC Twenty20 Tournament.\",\n",
    "    \"Comedian Bharti Singh will Host this show along with her husband writer Haarsh Limbachiyaa.\"\n",
    "    ]\n",
    "base_sent = \"hamod Wickramasuriya (born 27 May 1999) is a Sri Lankan cricketer. He made his Twenty20 debut on 15 January 2020, for Galle Cricket Club in the 2019–20 SLC Twenty20 Tournament.\"\n",
    "\n",
    "api_url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Wiki Study/1.0 (905899183@qq.com)\"\n",
    "}\n",
    "\n",
    "def search_wiki(text):\n",
    "    # 搜出10个结果\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": text,  # 精确匹配内容\n",
    "        \"srwhat\": \"text\",         # 指定在内容中搜索\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params, headers=headers)\n",
    "    search_results = response.json().get(\"query\", {}).get(\"search\", [])\n",
    "    \n",
    "    result = [{\"page_id\": result[\"pageid\"], \"title\": result[\"title\"]} for result in search_results]\n",
    "    return result\n",
    "\n",
    "for sent in sent_list:\n",
    "    result = search_wiki(sent)\n",
    "    print(result)\n",
    "\n",
    "# n_sent = len(sent_list)\n",
    "\n",
    "# sent_inputs = tokenizer(sent_list, return_tensors=\"pt\", padding=True)\n",
    "# base_sent_inputs = tokenizer(base_sent, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# sent_outputs = model(**sent_inputs)\n",
    "# base_sent_outputs = model(**base_sent_inputs)\n",
    "\n",
    "# sent_embeddings = sent_outputs.last_hidden_state[:, 0, :]   # cls\n",
    "# base_sent_embeddings = base_sent_outputs.last_hidden_state[:, 0, :]  # cls\n",
    "\n",
    "# cosine_similarities = F.cosine_similarity(base_sent_embeddings, sent_embeddings, dim=1)\n",
    "\n",
    "# for i in range(n_sent):\n",
    "#     print(f\"Similarity between base sentence and sentence {i + 1}: {cosine_similarities[i].item()}\")\n",
    "    \n",
    "# # sent_list 两两之间的相似度\n",
    "# cos_sim = F.cosine_similarity(sent_embeddings.unsqueeze(1), sent_embeddings.unsqueeze(0), dim=-1)\n",
    "# print(\"Similarity matrix:\")\n",
    "# print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每条句子搜索wiki\n",
    "import redis\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import base64\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "def text_decode(text):\n",
    "    # base64 解码\n",
    "    return base64.b64decode(text.encode()).decode()\n",
    "\n",
    "# 连接到 Redis 数据库\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, password='lyuredis579')\n",
    "\n",
    "prefix = 'wikisearch:'\n",
    "\n",
    "def search_wiki(text, max_proxies=10):\n",
    "    api_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    # 搜出10个结果\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": f'\"{text}\"',  # 精确匹配内容\n",
    "        \"srwhat\": \"text\",         # 指定在内容中搜索\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    proxy = {\n",
    "        \"https\": \"http://127.0.0.1:20172\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params, proxies=proxy)\n",
    "    time.sleep(random.uniform(0.05, 0.2))\n",
    "    search_results = response.json().get(\"query\", {}).get(\"search\", [])\n",
    "    \n",
    "    result = [{\"page_id\": result[\"pageid\"], \"title\": result[\"title\"]} for result in search_results]\n",
    "    return result\n",
    "\n",
    "def text_search_task(text, max=10):\n",
    "    key = prefix + text_encode(text)\n",
    "\n",
    "    if r.exists(key):\n",
    "        return json.loads(r.get(key))\n",
    "    else:\n",
    "        try:\n",
    "            result = search_wiki(text,max)\n",
    "            result = json.dumps(result, ensure_ascii=False)\n",
    "            r.set(key, result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying {text}: {e}\")\n",
    "            return None\n",
    "\n",
    "dataset_path = '../data/wiki1m_for_simcse.txt'\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    sent_list = file.read().splitlines()\n",
    "\n",
    "# 使用线程池进行并发请求\n",
    "max_workers = 30  # 设置并发线程数量\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(text_search_task, sent, max_workers) for sent in sent_list]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        future.result()  # 等待每个任务完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计查询为空的句子\n",
    "import redis\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 连接 Redis 数据库\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, password='lyuredis579')\n",
    "\n",
    "# 初始化计数器\n",
    "empty_list_count = 0\n",
    "total_count = 0\n",
    "prefix = 'wikisearch:'\n",
    "\n",
    "page_id_list = []\n",
    "\n",
    "# 遍历符合条件的键并统计内容为空列表的键数量\n",
    "for key in tqdm(r.scan_iter(prefix + '*')):\n",
    "    value = r.get(key)\n",
    "    # 检查值是否为空列表\n",
    "    if value is not None and value.decode() == '[]':\n",
    "        empty_list_count += 1\n",
    "    else:\n",
    "        page_id = [item['page_id'] for item in json.loads(value)]\n",
    "        page_id_list.extend(page_id)\n",
    "    total_count += 1\n",
    "print(f\"Keys with empty list content: {empty_list_count}\")\n",
    "print(f\"Total keys: {total_count}\")\n",
    "\n",
    "# 去重\n",
    "page_id_list = list(set(page_id_list))\n",
    "print(f\"Total page_id: {len(page_id_list)}\")\n",
    "\n",
    "r.set('page_id_list', json.dumps(page_id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import redis\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 连接 Redis 数据库\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, password='lyuredis579')\n",
    "\n",
    "# 初始化计数器\n",
    "empty_list_count = 0\n",
    "total_count = 0\n",
    "prefix = 'wikisearch:'\n",
    "\n",
    "page_id_list_key = 'page_id_list'\n",
    "\n",
    "if r.exists(page_id_list_key):\n",
    "    page_id_list = json.loads(r.get(page_id_list_key))\n",
    "else:\n",
    "    page_id_list = []\n",
    "\n",
    "    # 遍历符合条件的键并统计内容为空列表的键数量\n",
    "    for key in tqdm(r.scan_iter(prefix + '*'), desc='Scan keys'):\n",
    "        value = r.get(key)\n",
    "        # 检查值是否为空列表\n",
    "        if value is not None and value.decode() == '[]':\n",
    "            empty_list_count += 1\n",
    "        else:\n",
    "            page_id = [item['page_id'] for item in json.loads(value)]\n",
    "            page_id_list.extend(page_id)\n",
    "        total_count += 1\n",
    "    print(f\"Keys with empty list content: {empty_list_count}\")\n",
    "    print(f\"Total keys: {total_count}\")\n",
    "\n",
    "    # 去重\n",
    "    page_id_list = list(set(page_id_list))\n",
    "    print(f\"Total page_id: {len(page_id_list)}\")\n",
    "    r.set(page_id_list_key, json.dumps(page_id_list))\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=1, password='lyuredis579')\n",
    "\n",
    "proxy = {\n",
    "    \"https\": \"http://127.0.0.1:20171\"\n",
    "}\n",
    "\n",
    "def get_detailed_page_info(page_id, language='en'):\n",
    "    url = f\"https://{language}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"pageids\": page_id,\n",
    "        \"prop\": \"extracts|categories|info|images|pageprops|revisions\",\n",
    "        \"explaintext\": True,  # 返回纯文本格式\n",
    "        \"inprop\": \"url\",      # 包含页面的URL信息\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, proxies=proxy)\n",
    "        time.sleep(random.uniform(0.05, 0.2))\n",
    "        data = response.json()\n",
    "        \n",
    "        page_data = data['query']['pages'][str(page_id)]\n",
    "        \n",
    "        # 将详细信息提取到字典中\n",
    "        page_info = {\n",
    "            \"title\": page_data.get(\"title\"),\n",
    "            \"summary\": page_data.get(\"extract\"),  # 页面简介或全部内容\n",
    "            \"url\": page_data.get(\"fullurl\"),      # 页面URL\n",
    "            \"categories\": [cat['title'] for cat in page_data.get(\"categories\", [])],\n",
    "            \"images\": [img['title'] for img in page_data.get(\"images\", [])],  # 图片标题\n",
    "            \"wikidata_id\": page_data.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "        }\n",
    "        \n",
    "        return page_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying page ID {page_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "prefix = 'wikipage:'\n",
    "for page_id in tqdm(page_id_list, desc='Request page info'):\n",
    "    key = prefix + str(page_id)\n",
    "    if r.exists(key):\n",
    "        continue\n",
    "    page_info = get_detailed_page_info(page_id)\n",
    "    if page_info is not None:\n",
    "        r.set(key, json.dumps(page_info, ensure_ascii=False))\n",
    "\n",
    "def get_page_info(page_id):\n",
    "    key = prefix + str(page_id)\n",
    "    if not r.exists(key):\n",
    "        page_info = get_detailed_page_info(page_id)\n",
    "        if page_info is not None:\n",
    "            r.set(key, json.dumps(page_info, ensure_ascii=False))\n",
    "\n",
    "# 使用线程池进行并发请求\n",
    "max_workers = 30  # 设置并发线程数量\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(get_page_info, page_id) for page_id in page_id_list]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        future.result()  # 等待每个任务完成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询尝试\n",
    "import json\n",
    "import redis\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, password='lyuredis579')\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "page_dict = {}\n",
    "prefix = 'wikisearch:'\n",
    "\n",
    "input_file = '../data/wiki1m_for_simcse.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    sent_list = file.read().splitlines()\n",
    "\n",
    "sent_list = sent_list[:256]\n",
    "\n",
    "for sent in tqdm(sent_list):\n",
    "    key = prefix + text_encode(sent)\n",
    "    if r.exists(key):\n",
    "        page_dict[sent] = json.loads(r.get(key))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理数据分布\n",
    "import json\n",
    "import redis\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, password='lyuredis579')\n",
    "\n",
    "sent_id_list = r.get('page_id_list')\n",
    "sent_id_list = json.loads(sent_id_list)\n",
    "\n",
    "# 获取最大值和最小值\n",
    "max_value = np.max(sent_id_list)\n",
    "min_value = np.min(sent_id_list)\n",
    "\n",
    "def text_encode(text):\n",
    "    # base64 编码\n",
    "    return base64.b64encode(text.encode()).decode()\n",
    "\n",
    "\n",
    "with open('../data/wiki1m_for_simcse.txt', 'r', encoding='utf-8') as file:\n",
    "    sent_list = file.read().splitlines()\n",
    "\n",
    "data = []\n",
    "for i, sent in tqdm(enumerate(sent_list), desc='Query'):\n",
    "    key = 'wikisearch:' + text_encode(sent)\n",
    "    if r.exists(key):\n",
    "        page_id_list = json.loads(r.get(key))\n",
    "        page_id_list = [item['page_id'] for item in page_id_list]\n",
    "        data.append(page_id_list)\n",
    "    else:\n",
    "        data.append([])\n",
    "#     if i == 1000:\n",
    "#         break\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计batch内相关度\n",
    "import numpy as np\n",
    "\n",
    "bs = 256\n",
    "num_batches = len(data) // bs\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_data = data[i * bs: (i + 1) * bs]\n",
    "    batch_score = 0.0\n",
    "    for page_id_list in batch_data:\n",
    "        if len(page_id_list) == 0:\n",
    "            continue\n",
    "        score = len(set(page_id_list) & set(sent_id_list)) / len(set(page_id_list) | set(sent_id_list))\n",
    "        batch_score += score\n",
    "\n",
    "total_page_id_list = 0\n",
    "total_page_unique_list = 0\n",
    "for i in range(num_batches):\n",
    "    batch_data = data[i * bs: (i + 1) * bs]\n",
    "    batch_page_id_list = [item for sublist in batch_data for item in sublist]\n",
    "    batch_page_unique_list = list(set(batch_page_id_list))\n",
    "    total_page_id_list += len(batch_page_id_list)\n",
    "    total_page_unique_list += len(batch_page_unique_list)\n",
    "    if len(batch_page_unique_list) == 0:\n",
    "        continue\n",
    "    # 计算batch内的page重复率\n",
    "    print(f\"Batch {i}: {len(batch_page_unique_list) / len(batch_page_id_list)}\")\n",
    "print(f\"Total: {total_page_unique_list / total_page_id_list}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
