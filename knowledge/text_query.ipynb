{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于对语料库及wiki数据库进行查询进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算语料库的平均长度\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "file_path = 'data/wiki1m_knowledge.txt'\n",
    "\n",
    "# 读取文本文件，并计算每个句子的长度\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentence_word_counts = [len(line.strip().split()) for line in file]\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "sentence_word_counts_array = np.array(sentence_word_counts)\n",
    "\n",
    "# 计算中位数\n",
    "median = np.median(sentence_word_counts_array)\n",
    "\n",
    "# 计算众数\n",
    "mode = int(np.argmax(np.bincount(sentence_word_counts_array)))\n",
    "\n",
    "# 计算平均数\n",
    "mean = np.mean(sentence_word_counts_array)\n",
    "\n",
    "# 计算最大值和最小值\n",
    "max_value = np.max(sentence_word_counts_array)\n",
    "min_value = np.min(sentence_word_counts_array)\n",
    "\n",
    "print(\"中位数:\", median)\n",
    "print(\"众数:\", mode)\n",
    "print(\"平均数:\", mean)\n",
    "print(\"最大值:\", max_value)\n",
    "print(\"最小值:\", min_value)\n",
    "\n",
    "# 画频率分布图\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(sentence_word_counts, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert prompt test\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "path = '/pretrain_model/bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(path)\n",
    "model = BertModel.from_pretrained(path)\n",
    "text = \"Example sentence to be tokenized.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import os\n",
    "\n",
    "# 下载并加载BERT模型\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 将模型移动到指定的目录\n",
    "output_dir = 'pretrain_model/bert-base-uncased'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "# 加载NER模型\n",
    "tagger = SequenceTagger.load(\"ner\")\n",
    "\n",
    "# 创建一个句子\n",
    "sentence = Sentence(\"George Washington went to Washington.\")\n",
    "\n",
    "# 使用模型预测NER标签\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# 创建一个空的dict来存储结果\n",
    "entities_dict = {}\n",
    "\n",
    "# 遍历句子的所有实体，并存入字典\n",
    "for i, entity in enumerate(sentence.get_spans('ner')):\n",
    "    entities_dict[i] = {\n",
    "        \"text\": entity.text,                  # 实体文本\n",
    "        \"start_position\": entity.start_position,   # 实体起始位置\n",
    "        \"end_position\": entity.end_position,       # 实体结束位置\n",
    "        \"label\": entity.get_label('ner').value, # 实体的NER标签\n",
    "        \"confidence\": entity.score            # 实体的置信度得分\n",
    "    }\n",
    "\n",
    "# 输出存储的实体\n",
    "print(entities_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
